{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:96% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.2\n",
      "[06/24 23:46:31]\n",
      "1.4.0\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:96% !important; }</style>\"))\n",
    "from laplacian_pyramid import *\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.io import imread\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from skimage import img_as_ubyte\n",
    "import imageio\n",
    "import json\n",
    "import cv2\n",
    "import time\n",
    "import laplacian_pyramid\n",
    "from laplacian_pyramid import *\n",
    "from PIL import Image\n",
    "from pytorch3d.loss import (\n",
    "    mesh_laplacian_smoothing, \n",
    "    mesh_normal_consistency,\n",
    ")\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "# Util function for loading meshes\n",
    "from pytorch3d.io import load_objs_as_meshes, load_obj\n",
    "import math\n",
    "# Data structures and functions for rendering\n",
    "from pytorch3d.structures import Meshes, Textures, join_meshes_as_batch\n",
    "from pytorch3d.renderer import (\n",
    "    look_at_view_transform,\n",
    "    OpenGLPerspectiveCameras, \n",
    "    SfMPerspectiveCameras,\n",
    "    SfMOrthographicCameras,\n",
    "    PointLights, \n",
    "    BlendParams,\n",
    "    DirectionalLights,\n",
    "    Materials, \n",
    "    RasterizationSettings, \n",
    "    MeshRenderer, \n",
    "    MeshRasterizer,  \n",
    "    TexturedSoftPhongShader,\n",
    "    SoftSilhouetteShader,\n",
    "    look_at_rotation,\n",
    "    HardFlatShader\n",
    ")\n",
    "\n",
    "# add path for demo utils functions \n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "sys.path.append(os.path.abspath(''))\n",
    "\n",
    "print(torch.version.cuda)\n",
    "from datetime import datetime\n",
    "def now_str():\n",
    "    now = datetime.now()\n",
    "    month = str(now.month)\n",
    "    day = str(now.day)\n",
    "    hour = str(now.hour)\n",
    "    minute = str(now.minute)\n",
    "    sec = str(now.second)\n",
    "    \n",
    "    output = '[{:>02}/{:>02} {:>02}:{:>02}:{:>02}]'.format(month, day, hour, minute, sec)\n",
    "    return output\n",
    "def __output_log(path, strs):\n",
    "    if not os.path.exists(path):\n",
    "        with open(path, 'w+') as f:\n",
    "            f.write(strs)\n",
    "            f.close()\n",
    "    else:\n",
    "        with open(path, 'a+') as f:\n",
    "            f.write(strs)\n",
    "            f.close()\n",
    "print(now_str())\n",
    "print(torch.__version__)\n",
    "\n",
    "def reproject(params, vertices, distort=False):\n",
    "    R = params['R']\n",
    "    T = params['T']\n",
    "    fx = params['fx']\n",
    "    fy = params['fy']\n",
    "    cx = params['cx']\n",
    "    cy = params['cy']\n",
    "\n",
    "    E = np.array([\n",
    "        [R[0,0], R[0,1], R[0,2], T[0]], \n",
    "        [R[1,0], R[1,1], R[1,2], T[1]], \n",
    "        [R[2,0], R[2,1], R[2,2], T[2]], \n",
    "        [0, 0, 0, 1]]).astype('double')\n",
    "    \n",
    "    if distort:\n",
    "        k1 = params['k1']\n",
    "        k2 = params['k2']\n",
    "        k3 = params['k3']\n",
    "        p1 = params['p1']\n",
    "        p2 = params['p2']\n",
    "        \n",
    "    img_pts = []\n",
    "    for i in range(len(vertices)):\n",
    "        v = np.array(vertices[i])\n",
    "\n",
    "        # extrinsics\n",
    "        v4 = E.dot(np.array([v[0], v[1], v[2], 1]).astype('double'))\n",
    "        xp = v4[0] / v4[2]\n",
    "        yp = v4[1] / v4[2]\n",
    "\n",
    "        if distort:\n",
    "            # intrinsics\n",
    "            r2 = xp**2 + yp**2\n",
    "            ## radial\n",
    "            radial_dist = 1 + k1*(r2) + k2*(r2*r2) + k3*(r2*r2*r2)\n",
    "\n",
    "            ## tangential\n",
    "            tan_x = p2 * (r2 + 2.0 * xp * xp) + 2.0 * p1 * xp * yp\n",
    "            tan_y = p1 * (r2 + 2.0 * yp * yp) + 2.0 * p2 * xp * yp\n",
    "\n",
    "            xp = xp * radial_dist + tan_x\n",
    "            yp = yp * radial_dist + tan_y\n",
    "            \n",
    "        u = fx * xp + cx\n",
    "        v = fy * yp + cy\n",
    "        pr = 1\n",
    "        nr = 0\n",
    "        if (-4000*nr < u and u < pr*4000) and (-2160*nr < v and v < pr*2160):\n",
    "            img_pts.append(np.array([u, v]))\n",
    "    img_pts = np.array(img_pts)\n",
    "    return img_pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.is_available(): True\n",
      "----- torch.cuda.empty_cache() -----\n",
      "torch.cuda.memory_reserved(): 0.00 Mb\n",
      "torch.cuda.memory_allocated(): 0.00 Mb\n",
      "torch.cuda.memory_cached(): 0.00 Mb\n"
     ]
    }
   ],
   "source": [
    "print('torch.cuda.is_available():',torch.cuda.is_available())\n",
    "print('----- torch.cuda.empty_cache() -----')\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "bytes_reserved = torch.cuda.memory_reserved()\n",
    "print('torch.cuda.memory_reserved(): {:,.2f} Mb'.format(bytes_reserved * 0.000001))\n",
    "bytes_alloc = torch.cuda.memory_allocated()\n",
    "print('torch.cuda.memory_allocated(): {:,.2f} Mb'.format(bytes_alloc * 0.000001))\n",
    "bytes_cached = torch.cuda.memory_cached()\n",
    "print('torch.cuda.memory_cached(): {:,.2f} Mb'.format(bytes_cached * 0.000001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meshes:\n",
      "  ./22_data/200619_SilFittedMesh_vtAdded\\03052.obj\n",
      "  ./22_data/200619_SilFittedMesh_vtAdded\\03067.obj\n",
      "  ./22_data/200619_SilFittedMesh_vtAdded\\04735.obj\n",
      "  ./22_data/200619_SilFittedMesh_vtAdded\\06250.obj\n",
      "  ./22_data/200619_SilFittedMesh_vtAdded\\06550.obj\n",
      "80 renderes\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "cams = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P']\n",
    "cam_path = './22_data/cam_params.json'\n",
    "mesh_dir = './22_data/200619_SilFittedMesh_vtAdded'\n",
    "img_dir = r'Z:\\2019_12_13_Lada_Capture\\200512_MeshSparseFitted\\images'\n",
    "# img_names = ['03052', '03990', '04917', '06950']\n",
    "# img_names = ['03052', '03990', '04917', '06950']\n",
    "# img_names = ['03052', '04917']\n",
    "img_names = ['03052', '03067', '04735', '06550', '06250']\n",
    "clean_plate_dir = './22_data/CleanPlates/undistorted'\n",
    "texturemap_path = r'./16g_data/input/5frames/200621_L1_yesMask/texturemap.npy'\n",
    "\n",
    "texturemap_shape = (1024, 1024, 1)\n",
    "image_size = 1080\n",
    "\n",
    "# input image size\n",
    "actual_img_shape = (2160, 4000)\n",
    "\n",
    "mesh_paths_ = glob.glob(mesh_dir + '/*.obj')\n",
    "mesh_paths = []\n",
    "print('Meshes:')\n",
    "for p in mesh_paths_:\n",
    "    for img_name in img_names:\n",
    "        if img_name in p:\n",
    "            mesh_paths.append(p)\n",
    "            print(' ', p)\n",
    "n_forwards = len(img_names)*len(cams)\n",
    "print('{} renderes'.format(n_forwards))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_img_shape: (2160, 4000)\n",
      "16 : dict_keys(['K', 'dist', 'R', 'T', 'fx', 'fy', 'cx', 'cy'])\n",
      "dict_keys(['R', 'T', 'fl', 'pp'])\n"
     ]
    }
   ],
   "source": [
    "def load_cameras(cam_path, device, actual_img_shape):\n",
    "    print('actual_img_shape:',actual_img_shape)\n",
    "    h = actual_img_shape[0]\n",
    "    w = actual_img_shape[1]\n",
    "    img_size = min(w, h)\n",
    "    \n",
    "    # load cameras\n",
    "    cams = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P']\n",
    "\n",
    "    with open(cam_path, 'r') as f:\n",
    "        j = json.load(f)\n",
    "        camera_params = j['cam_params']\n",
    "\n",
    "    cam_params = []\n",
    "    Rs, Ts, focal_lengths, principal_points = [], [], [], []\n",
    "    for cam_idx, cam in enumerate(cams):\n",
    "        cam_param = camera_params[str(cam_idx)]\n",
    "        # for undistortion\n",
    "        fx = cam_param['fx']\n",
    "        fy = cam_param['fy']\n",
    "        cx = cam_param['cx']\n",
    "        cy = cam_param['cy']\n",
    "        k1 = cam_param['k1']\n",
    "        k2 = cam_param['k2']\n",
    "        k3 = cam_param['k3']\n",
    "        p1 = cam_param['p1']\n",
    "        p2 = cam_param['p2']\n",
    "        \n",
    "        rvec = np.float32(cam_param['rvec'])\n",
    "        T = np.float32(cam_param['tvec'])\n",
    "        R, _ = cv2.Rodrigues(rvec)\n",
    "        Rs.append(R.T)\n",
    "        Ts.append(T)\n",
    "        \n",
    "        cx_corrected = cx*2/img_size - w/img_size\n",
    "        cy_corrected = cy*2/img_size - h/img_size\n",
    "        fx_corrected = fx*2/img_size\n",
    "        fy_corrected = fy*2/img_size\n",
    "        principal_point = np.array([cx_corrected, cy_corrected]).astype(np.float32)\n",
    "        focal_length = np.array([fx_corrected, fy_corrected]).astype(np.float32)\n",
    "        focal_lengths.append(focal_length)\n",
    "        principal_points.append(principal_point)\n",
    "\n",
    "        K = np.float32([[fx, 0, cx], [0, fy, cy], [0, 0, 1]])\n",
    "        dist = np.float32([k1, k2, p1, p2, k3])\n",
    "        cam_params.append({'K': K, 'dist': dist, 'R': R, 'T': T, 'fx': fx, 'fy': fy, 'cx': cx, 'cy': cy})\n",
    "    \n",
    "    R_torch = torch.from_numpy(np.array(Rs).astype(np.float32))\n",
    "    T_torch = torch.from_numpy(np.array(Ts).astype(np.float32))\n",
    "    focal_length = torch.from_numpy(np.array(focal_lengths).astype(np.float32))\n",
    "    principal_point = torch.from_numpy(np.array(principal_points).astype(np.float32))\n",
    "    out_for_torch = {'R': R_torch, 'T': T_torch, 'fl': focal_length, 'pp': principal_point}\n",
    "    return cam_params, out_for_torch\n",
    "\n",
    "cam_params, cams_torch = load_cameras(cam_path, device, actual_img_shape)\n",
    "print(len(cam_params), ':', cam_params[0].keys())\n",
    "print(cams_torch.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load target, bg images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z:\\2019_12_13_Lada_Capture\\200512_MeshSparseFitted\\images\\03052\n",
      "Z:\\2019_12_13_Lada_Capture\\200512_MeshSparseFitted\\images\\03067\n",
      "Z:\\2019_12_13_Lada_Capture\\200512_MeshSparseFitted\\images\\04735\n",
      "Z:\\2019_12_13_Lada_Capture\\200512_MeshSparseFitted\\images\\06550\n",
      "Z:\\2019_12_13_Lada_Capture\\200512_MeshSparseFitted\\images\\06250\n",
      "img_refs_undistorted: dict_keys(['03052', '03067', '04735', '06550', '06250'])\n",
      "img_refs: dict_keys(['03052', '03067', '04735', '06550', '06250'])\n",
      "03052 16\n",
      "03067 16\n",
      "04735 16\n",
      "06550 16\n",
      "06250 16\n"
     ]
    }
   ],
   "source": [
    "def load_images(img_dir, img_names):\n",
    "    image_refs_out = {}\n",
    "    crops_out = {}\n",
    "    \n",
    "    w = 2160 / 2\n",
    "    for img_name in img_names:\n",
    "        path = img_dir + '\\\\{}'.format(img_name)\n",
    "        print(path)\n",
    "        img_paths = sorted(glob.glob(path + '/*.png'))\n",
    "        image_refs_undistort = []\n",
    "        for i, path in enumerate(img_paths):\n",
    "            img = cv2.imread(path, cv2.IMREAD_GRAYSCALE).astype(np.float32) / 255.0\n",
    "#             img = cv2.imread(path).astype(np.float32) / 255.0\n",
    "#             img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            image_refs_undistort.append(img)\n",
    "\n",
    "        image_refs_cropped = []\n",
    "        for i in range(len(image_refs_undistort)):\n",
    "            image = image_refs_undistort[i]\n",
    "            cx = image.shape[1] / 2\n",
    "\n",
    "            image = image_refs_undistort[i]\n",
    "            img = image[:, int(cx-w):int(cx+w)]\n",
    "            img = cv2.resize(img, (image_size, image_size))\n",
    "            img = cv2.flip(img, -1)\n",
    "            image_refs_cropped.append(img)\n",
    "        image_refs_out[img_name] = image_refs_undistort\n",
    "        crops_out[img_name] = image_refs_cropped\n",
    "    return image_refs_out, crops_out\n",
    "\n",
    "img_refs_undistorted, img_refs = load_images(img_dir, img_names)\n",
    "print('img_refs_undistorted:', img_refs_undistorted.keys())\n",
    "print('img_refs:', img_refs.keys())\n",
    "for k, v in img_refs.items():\n",
    "    print(k, len(v))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_plates_original: (2160, 4000)\n"
     ]
    }
   ],
   "source": [
    "def load_clean_plates(img_dir, cam_params):\n",
    "    img_paths = sorted(glob.glob(img_dir + '/*.PNG'))\n",
    "    images0 = []\n",
    "    images_undistort = []\n",
    "    for i, path in enumerate(img_paths):\n",
    "        # img = imageio.imread(path)\n",
    "        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE).astype(np.float32) / 255.0\n",
    "        images0.append(img)\n",
    "        images_undistort.append(img)\n",
    "\n",
    "    w = 2160 / 2\n",
    "    clean_plates_cropped = []\n",
    "    for i in range(len(images_undistort)):\n",
    "        image = images_undistort[i]\n",
    "        cx = image.shape[1] / 2\n",
    "\n",
    "        image = images_undistort[i].astype(np.float32)\n",
    "        img = image[:, int(cx-w):int(cx+w)]\n",
    "        img = cv2.resize(img, (image_size, image_size))\n",
    "        img = cv2.flip(img, -1)\n",
    "        # img = np.dstack([img, img, img])\n",
    "        clean_plates_cropped.append(img)\n",
    "    \n",
    "    return images0, images_undistort, clean_plates_cropped \n",
    "\n",
    "clean_plates_original, clean_plates_undistort, clean_plates = load_clean_plates(clean_plate_dir, cam_params)\n",
    "print('clean_plates_original:', clean_plates_original[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.dnormals: torch.Size([6750, 1]) , nverts: torch.Size([6750, 3])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'LaplacianPyramid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-f049283bd80d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[0mn_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mn_forwards\u001b[0m \u001b[1;33m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m \u001b[1;31m# texturemap_path = None\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 378\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModel_normal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mimg_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtexturemap_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtexturemap_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtexturemap_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtexturemap_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcam_params\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcams_torch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage_refs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mimg_refs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmesh_paths\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmesh_paths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclean_plates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclean_plates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_batch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    379\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Init'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-f049283bd80d>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, device, **kwargs)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m         \u001b[1;31m# laplacian pyramids\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlap_pyr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLaplacianPyramid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_res\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchannels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.45\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlaplacian_refs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mimg_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_tensor_batch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage_refs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'LaplacianPyramid' is not defined"
     ]
    }
   ],
   "source": [
    "class Model_normal(nn.Module):\n",
    "    def __init__(self, device, **kwargs):\n",
    "        super().__init__()\n",
    "        self.batch_size = kwargs.get('batch_size', None)\n",
    "        \n",
    "        self.device = device\n",
    "        self.cams = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P']\n",
    "        self.n_cams = len(self.cams)\n",
    "        self.image_size = kwargs.get('image_size', None)\n",
    "        image_refs = kwargs.get('image_refs', None).copy()\n",
    "        self.image_refs = {}\n",
    "        \n",
    "        for img_name, img_list in image_refs.items():\n",
    "            # (16, 1080, 1080, 3)\n",
    "            imgs_torch = torch.from_numpy(np.array(img_list).astype(np.float32)).to(self.device)\n",
    "            self.image_refs[img_name] = imgs_torch\n",
    "            \n",
    "        # set clean_plates:\n",
    "        clean_plates = kwargs.get('clean_plates', None)\n",
    "        if clean_plates is not None:\n",
    "            # (16, 1080, 1080, 1)\n",
    "            self.clean_plates = torch.from_numpy(np.array(clean_plates).astype(np.float32)).unsqueeze(-1).to(self.device)\n",
    "        \n",
    "        # load texturemaps: [0.0, 1.0] float\n",
    "        texturemap_path = kwargs.get('texturemap_path', None)\n",
    "        texturemap_shape = kwargs.get('texturemap_shape', None)\n",
    "        if texturemap_path is not None:\n",
    "            if '.png' in texturemap_path:\n",
    "                texturemap = imageio.imread(texturemap_path) / 255.0\n",
    "            elif '.npy' in texturemap_path:\n",
    "                texturemap = (np.load(texturemap_path).squeeze() * 255).astype(np.uint8)\n",
    "#                 texturemap = cv2.GaussianBlur(texturemap, (7, 7), 3)\n",
    "                texturemap = (texturemap/255.0).astype(np.float32)\n",
    "            texture_map_torch = torch.from_numpy(texturemap).unsqueeze(0).unsqueeze(-1).float()\n",
    "        else:\n",
    "            texture_map_torch = torch.from_numpy(np.ones((1, texturemap_shape[0], texturemap_shape[1], 1)).astype(np.float32))\n",
    "        self.texture_map = nn.Parameter(texture_map_torch.to(self.device), requires_grad=True)\n",
    "\n",
    "        # batch_size\n",
    "        self.batch_dict = {'img_name': [], 'cam_idx': [], 'mesh_idx': []}\n",
    "        i = 0\n",
    "        for img_name in self.image_refs.keys():\n",
    "            for cam_idx in range(self.n_cams):\n",
    "                self.batch_dict['img_name'].append(img_name)\n",
    "                self.batch_dict['cam_idx'].append(cam_idx)\n",
    "                self.batch_dict['mesh_idx'].append(i)\n",
    "            i += 1\n",
    "\n",
    "        # ====================\n",
    "        # lights & renderers\n",
    "        # ====================\n",
    "        xyz = torch.from_numpy(np.float32([0, 0, 2000]))\n",
    "        \n",
    "        # fixed light brightness\n",
    "#         diffuse = 1.2505254745483398\n",
    "#         ambient = 0.5083667039871216\n",
    "#         specular = 0.0\n",
    "        diffuse = 0.0\n",
    "        ambient = 1.0\n",
    "        specular = 0.0\n",
    "        s = specular*torch.from_numpy(np.ones((1, 3)).astype(np.float32)).to(self.device)\n",
    "        d = diffuse *torch.from_numpy(np.ones((1, 3)).astype(np.float32)).to(self.device)\n",
    "        a = ambient *torch.from_numpy(np.ones((1, 3)).astype(np.float32)).to(self.device)\n",
    "        light = PointLights(device=self.device, location=xyz, specular_color=s, ambient_color=a, diffuse_color=d)\n",
    "        light.specular_color = s\n",
    "        light.diffuse_color = d\n",
    "        light.ambient_color = a\n",
    "        shader = TexturedSoftPhongShader(\n",
    "                    device=self.device, \n",
    "                    cameras=None,\n",
    "                    lights=light,\n",
    "                    blend_params=BlendParams(sigma=1e-4, gamma=1e-4)\n",
    "                )\n",
    "        # init renderers\n",
    "        self.renderer = self._init_renderer(shader)\n",
    "        \n",
    "        # camera batches\n",
    "        self.cam_params = kwargs.get('cam_params', None)\n",
    "        self.n_batch = kwargs.get('n_batch', None)\n",
    "        self.cam_batches = self._init_camera_batches(self.cam_params, batch_dict=self.batch_dict, n_batch=self.n_batch, batch_size=self.batch_size)\n",
    "\n",
    "        # set mesh\n",
    "        mesh_paths = kwargs.get('mesh_paths', None)\n",
    "        self.meshes = self._load_meshes_list(self.device, mesh_paths=mesh_paths, texture_map=texture_map_torch.float())\n",
    "        \n",
    "        # vertex deformations\n",
    "        nverts = self.meshes[0].verts_normals_packed()\n",
    "        dnormal = torch.from_numpy(np.zeros((nverts.shape[0], 1), dtype=np.float32))\n",
    "        self.dnormals = nn.Parameter(dnormal.to(self.device), requires_grad=True)\n",
    "        print('self.dnormals:', self.dnormals.shape, ', nverts:', nverts.shape)\n",
    "        \n",
    "        # laplacian pyramids\n",
    "        self.lap_pyr = LaplacianPyramid(self.device, self.image_size, n_res=3, channels=1, kernel_size=5, alpha=0.45)\n",
    "        self.laplacian_refs = {}\n",
    "        for img_name, img_tensor_batch in self.image_refs.items():\n",
    "            lap_list, _ = self.lap_pyr.build_laplacian_pyramid(img_tensor_batch.unsqueeze(-1))\n",
    "            self.laplacian_refs[img_name] = lap_list\n",
    "        \n",
    "    def _init_renderer(self, shader):\n",
    "        raster_settings = RasterizationSettings(\n",
    "            image_size=self.image_size, \n",
    "            blur_radius=0.0, \n",
    "            faces_per_pixel=2, \n",
    "            bin_size = 0, # this setting controls whether naive or coarse-to-fine rasterization is used\n",
    "            max_faces_per_bin = None  # this setting is for coarse rasterization\n",
    "        )\n",
    "        renderer = MeshRenderer(\n",
    "                rasterizer=MeshRasterizer(\n",
    "                    cameras=None,\n",
    "                    raster_settings=raster_settings\n",
    "                ),\n",
    "                shader=shader\n",
    "            )\n",
    "        return renderer\n",
    "    \n",
    "    def deform_meshes(self):\n",
    "        self.deformed_meshes = []\n",
    "        for i in range(len(self.meshes)):\n",
    "            deform_verts = self.dnormals * self.meshes[i].verts_normals_packed()\n",
    "            self.deformed_meshes.append(self.meshes[i].offset_verts(deform_verts))\n",
    "    \n",
    "    def set_mesh_texturemap(self):\n",
    "        for i in range(len(self.meshes)):\n",
    "            self.meshes[i].textures_maps_padded = self.texture_map.float()\n",
    "        \n",
    "    def forward(self, batch_idx, learn_texturemap: bool, learn_deform: bool):\n",
    "        losses = {'total': 0.0, 'lap_pyr': 0.0, 'laplacian': 0.0}\n",
    "        \n",
    "        self.dnormals.requires_grad = learn_deform\n",
    "        self.texture_map.requires_grad = learn_texturemap\n",
    "        \n",
    "        # ==================================================================================== #\n",
    "        # minibatch training\n",
    "        i0 = batch_idx*self.batch_size\n",
    "        i1 = i0 + self.batch_size\n",
    "\n",
    "        cam_indices = self.batch_dict['cam_idx'][i0:i1]\n",
    "        mesh_indices = self.batch_dict['mesh_idx'][i0:i1]\n",
    "        img_names = self.batch_dict['img_name'][i0:i1]\n",
    "        \n",
    "        cam_batch = self.cam_batches[batch_idx]\n",
    "        meshes = [self.deformed_meshes[i] for i in mesh_indices]\n",
    "        meshes = join_meshes_as_batch(meshes)\n",
    "        image_cur = self.renderer(meshes_world=meshes, texture_maps=self.texture_map, cameras=cam_batch)\n",
    "        \n",
    "        # shape (batch_size, W, H)\n",
    "        bgs = self.clean_plates[cam_indices].squeeze()\n",
    "\n",
    "        # merge fg, bg, single channel\n",
    "        image_cur_gray = self._merge_fg_bg(image_cur, bgs)\n",
    "        lap_list, _ = self.lap_pyr.build_laplacian_pyramid(image_cur_gray.unsqueeze(-1))\n",
    "        \n",
    "        # [0, 0.1] float32\n",
    "        image_refs = torch.stack([self.image_refs[img_names[i]][cam_indices[i]] for i in range(self.batch_size)]).to(self.device)\n",
    "        #print('image_cur:', image_cur.shape, ', bgs:', bgs.shape, ', image_cur_gray:', image_cur_gray.shape, ', image_refs:' ,image_refs.shape)\n",
    "        \n",
    "        # l_pixel = torch.mean(torch.abs(image_cur_gray - image_refs))\n",
    "        l_lap_pyr = None\n",
    "        for i in range(self.batch_size):\n",
    "            img_name = img_names[i]\n",
    "            for res_idx in range(len(lap_list)):\n",
    "                lap_curr = lap_list[res_idx]\n",
    "                lap_refs = self.laplacian_refs[img_name][res_idx][i0:i1]\n",
    "                if l_lap_pyr is None:\n",
    "                    l_lap_pyr = torch.mean(torch.abs(lap_curr - lap_refs))\n",
    "                else:\n",
    "                    l_lap_pyr += torch.mean(torch.abs(lap_curr - lap_refs))\n",
    "                \n",
    "        # l_pixel = torch.mean(torch.abs(image_cur_gray - image_refs))\n",
    "        \n",
    "        l_laplacian = 0.001*mesh_laplacian_smoothing(meshes, method=\"cotcurv\")\n",
    "\n",
    "        loss = l_pixel + l_lap_pyr + l_laplacian\n",
    "        # losses['pixel'] += (l_pixel.data / self.batch_size / self.n_batch)\n",
    "        losses['lap_pyr'] += (l_lap_pyr.data / self.batch_size / self.n_batch)\n",
    "        losses['laplacian'] += (l_laplacian.data / self.batch_size / self.n_batch)\n",
    "        losses['total'] += (loss.data / self.batch_size / self.n_batch)\n",
    "        images_out = {'currents_with_bg': image_cur_gray, 'currents': image_cur, 'targets': image_refs}\n",
    "        # ==================================================================================== #\n",
    "        \n",
    "        return images_out, loss, losses\n",
    "    \n",
    "    \n",
    "    def unit_spherical_to_cartesian(self, light_coords):\n",
    "        \"\"\"\n",
    "        z-up, phi w.r.t. +z axis, theta w.r.t. +x axis\n",
    "        \"\"\"\n",
    "        thetas = light_coords[0, :]\n",
    "        phis = light_coords[1, :]\n",
    "        radiuses = light_coords[2, :]\n",
    "        \n",
    "        r = radiuses * torch.sin(phis)\n",
    "        z = radiuses * torch.cos(phis)\n",
    "        x = r * torch.cos(thetas)\n",
    "        y = r * torch.sin(thetas)\n",
    "        \n",
    "        xyz = torch.stack([x, y, z], dim=1)\n",
    "        return xyz    \n",
    "        \n",
    "    def _merge_fg_bg(self, fg, bg):\n",
    "        \"\"\"\n",
    "        fg: mesh rendering. [N, W, H, 4]: [0, 1.0] float\n",
    "        bg: clean plate. [N, W, H, 1]: [0, 1.0] float\n",
    "        out: [N, W, H, 4]\n",
    "        \"\"\"\n",
    "        fg_single_channel = torch.mean(fg[..., :3], dim=-1)\n",
    "        out = torch.where(fg[..., 3] > 0.0, fg_single_channel, bg)\n",
    "        if len(out.shape) < 3:\n",
    "            out = out.unsqueeze(0)\n",
    "        return out\n",
    "\n",
    "    def get_gpu_stats(self, output_str=True):\n",
    "        mb_reserved = torch.cuda.memory_reserved() * 0.000001\n",
    "        mb_alloc = torch.cuda.memory_allocated() * 0.000001\n",
    "        mb_alloc_max = torch.cuda.max_memory_allocated() * 0.000001\n",
    "        mb_cached = torch.cuda.memory_cached() * 0.000001\n",
    "        mb_cached_max = torch.cuda.max_memory_cached() * 0.000001\n",
    "        \n",
    "        if output_str:\n",
    "            return 'alloc={:,.0f}MB | cached={:,.0f}MB | reserved={:,.0f}MB'.format(mb_alloc, mb_cached, mb_reserved)\n",
    "        else:\n",
    "            return mb_alloc, mb_cached, mb_reserved\n",
    "    \n",
    "    def _load_meshes_list(self, device, mesh_paths, texture_map):\n",
    "        meshes_list = []\n",
    "        for path in mesh_paths:\n",
    "            verts, faces, aux = load_obj(path)\n",
    "            faces_idx = faces.verts_idx\n",
    "\n",
    "            verts_uvs = aux.verts_uvs[None, ...].to(device)  # (1, V, 2)\n",
    "            faces_uvs = faces.textures_idx[None, ...].to(device)  # (1, F, 3)\n",
    "            textures = Textures(maps=texture_map, faces_uvs=faces_uvs, verts_uvs=verts_uvs)\n",
    "\n",
    "            mesh = Meshes(\n",
    "                verts=[verts.to(device)],   \n",
    "                faces=[faces_idx.to(device)], \n",
    "                textures=textures.to(device)\n",
    "            )\n",
    "\n",
    "            meshes_list.append(mesh)\n",
    "        return meshes_list\n",
    "    \n",
    "    \n",
    "    def _convert_mesh_into_batches(self, meshes):\n",
    "        mesh_batches = []\n",
    "        for batch_idx in range(self.n_batch):\n",
    "            i0 = batch_idx*self.batch_size\n",
    "            i1 = i0 + self.batch_size\n",
    "            meshes_join = []\n",
    "            for i in self.batch_dict['mesh_idx'][i0:i1]:\n",
    "                meshes_join.append(meshes[i])\n",
    "            mesh_batch = join_meshes(meshes_join, include_textures=True)\n",
    "            mesh_batches.append(mesh_batch)\n",
    "        return mesh_batches\n",
    "    \n",
    "    def save_parameters(self, out_path):\n",
    "        deform_verts = self.deform_verts.detach().cpu().numpy()\n",
    "        np.save(out_path, deform_verts)\n",
    "        print('Parameters saved:', out_path)\n",
    "        \n",
    "    def load_parameters(self, in_path):\n",
    "        self.deform_verts = nn.Parameter(torch.from_numpy(np.load(in_path)).to(self.device))\n",
    "        print('Parameters loaded: {}'.format(self.deform_verts.shape))\n",
    "        \n",
    "    def export_obj(self, out_dir, vt_path=None, export_texturemap=False, fname_suffix=''):\n",
    "        out_name = 'mesh_deformed'\n",
    "\n",
    "        # export texturemap\n",
    "        if export_texturemap:\n",
    "            tm = model.texture_map.clone().squeeze().detach().cpu().numpy()\n",
    "            if tm is not None:\n",
    "                tm = (255.0*tm).astype(np.uint8)\n",
    "                im = Image.fromarray(tm)\n",
    "                out_path = out_dir + '/texturemap_learned.png'\n",
    "                im.save(out_path, dpi=(600, 600))\n",
    "                print('texturemap saved to:', out_path)\n",
    "\n",
    "        # export mtl\n",
    "        with open(out_dir + '/{}.mtl'.format(out_name), 'w+') as f:\n",
    "            f.write('map_Kd texturemap_learned.png\\n')\n",
    "            f.write('newmtl None\\n')\n",
    "            f.write('Ns 500\\n')\n",
    "            f.write('Ka 0.8 0.8 0.8\\n')\n",
    "            f.write('Kd 0.8 0.8 0.8\\n')\n",
    "            f.write('Ks 0.8 0.8 0.8\\n')\n",
    "            f.write('d 1\\n')\n",
    "            f.write('illum 2')\n",
    "            f.close()\n",
    "\n",
    "        vt_lines = []\n",
    "        f_lines = []\n",
    "        if vt_path is not None:\n",
    "            with open(vt_path, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "                for l in lines:\n",
    "                    v = l.split(' ')\n",
    "                    if v[0] == 'vt':\n",
    "                        vt_lines.append(l)\n",
    "                    elif v[0] == 'f':\n",
    "                        f_lines.append(l)\n",
    "\n",
    "        out_name = 'mesh_deformed'\n",
    "        for mesh_idx in range(len(self.meshes)):\n",
    "            out_path = out_dir + '/{}_{}{}.obj'.format(out_name, mesh_idx, fname_suffix)\n",
    "            dverts = self.dnormals * self.meshes[mesh_idx].verts_normals_packed()\n",
    "            deformed_mesh = self.meshes[mesh_idx].offset_verts(dverts)\n",
    "\n",
    "            verts = deformed_mesh.verts_packed()\n",
    "            faces = deformed_mesh.faces_packed()\n",
    "            vnormals = deformed_mesh.verts_normals_list()[0]\n",
    "            fnormals = deformed_mesh.faces_normals_list()[0]\n",
    "\n",
    "            assert(faces.shape[0] == fnormals.shape[0])\n",
    "            assert(vnormals.shape[0] == verts.shape[0])\n",
    "\n",
    "            with open(out_path, 'w+') as f:\n",
    "                f.write('# OBJ file created by Hyojoon Park.\\n')\n",
    "                f.write('###########################\\n')\n",
    "                f.write('# Vertices:       {}\\n'.format(verts.shape[0]))\n",
    "                f.write('# Vertex normals: {}\\n'.format(vnormals.shape[0]))\n",
    "                f.write('# Faces:          {}\\n'.format(faces.shape[0]))\n",
    "                f.write('###########################\\n')\n",
    "                f.write('mtllib {}.mtl\\n'.format(out_name))\n",
    "                for i in range(verts.shape[0]):\n",
    "                    f.write('vn {} {} {}\\n'.format(vnormals[i][0], vnormals[i][1], vnormals[i][2]))\n",
    "                    f.write('v {} {} {}\\n'.format(verts[i][0], verts[i][1], verts[i][2]))\n",
    "                    \n",
    "                for vtl in vt_lines:\n",
    "                    f.write(vtl)\n",
    "                    \n",
    "                if len(f_lines) > 0:\n",
    "                    for fl in f_lines:\n",
    "                        f.write(fl)\n",
    "                else:\n",
    "                    for i in range(faces.shape[0]):\n",
    "                        f.write(\"f\")\n",
    "                        face = faces[i, :]\n",
    "                        for fi in range(face.shape[0]):\n",
    "                            f.write(' {0:.0f}//{0:.0f}//{0:.0f}'.format(face[fi] + 1, fnormals[fi] + 1))\n",
    "        #                     f.write(' {0:.0f}'.format(face[fi]))\n",
    "                        f.write(\"\\n\")\n",
    "\n",
    "            print('[{}/{}] Obj exported to: {}'.format(mesh_idx+1, len(self.meshes), out_path))\n",
    "        \n",
    "    def _init_cameras(self, cam_torch):\n",
    "        n_cams = len(cam_torch['T'])\n",
    "        assert(n_cams == self.n_cams)\n",
    "        Rs = torch.empty(n_cams, 3, 3)\n",
    "        Ts = torch.empty(n_cams, 3)\n",
    "        fls = torch.empty(n_cams, 2)\n",
    "        pps = torch.empty(n_cams, 2)\n",
    "        for cam_idx in range(n_cams):\n",
    "            fls[cam_idx] = cam_torch['fl'][cam_idx]\n",
    "            pps[cam_idx] = cam_torch['pp'][cam_idx]\n",
    "            Rs[cam_idx] = cam_torch['R'][cam_idx]\n",
    "            Ts[cam_idx] = cam_torch['T'][cam_idx]\n",
    "        cameras = SfMPerspectiveCameras(device=self.device, R=Rs, T=Ts, principal_point=pps, focal_length=fls)\n",
    "        return cameras\n",
    "    \n",
    "    def _init_camera_batches(self, cam_torch, batch_dict, n_batch, batch_size):\n",
    "        cams = []\n",
    "        for batch_idx in range(n_batch):\n",
    "            i0 = batch_idx*batch_size\n",
    "            i1 = i0 + batch_size\n",
    "            cam_indices = batch_dict['cam_idx'][i0:i1]\n",
    "            R = cam_torch['R'][cam_indices]\n",
    "            T = cam_torch['T'][cam_indices]\n",
    "            focal_length = cam_torch['fl'][cam_indices]\n",
    "            principal_point = cam_torch['pp'][cam_indices]\n",
    "            cameras = SfMPerspectiveCameras(device=self.device, R=R, T=T, principal_point=principal_point, focal_length=focal_length)\n",
    "            cams.append(cameras)\n",
    "        return cams\n",
    "model = None\n",
    "torch.cuda.empty_cache()\n",
    "batch_size = 1\n",
    "n_batch = n_forwards // batch_size\n",
    "# texturemap_path = None\n",
    "model = Model_normal(device, img_dir=img_dir, texturemap_path=texturemap_path, texturemap_shape=texturemap_shape, cam_params=cams_torch, image_refs=img_refs, mesh_paths=mesh_paths, image_size=image_size, clean_plates=clean_plates, batch_size=batch_size, n_batch=n_batch)\n",
    "print('Init')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(img_refs['03052'])\n",
    "t = np.ones((1, 10, 10, 4))\n",
    "tt = np.mean(t, axis=-1)\n",
    "print(t.shape)\n",
    "print(tt.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check target images, clean plates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "img_name = '03052'\n",
    "cam_idx = 0\n",
    "img1 = img_refs_undistorted[img_name][cam_idx]\n",
    "img2 = img_refs[img_name][cam_idx]\n",
    "print('Image original =========')\n",
    "print(' ', img1.shape, ',', np.max(img1), ',', img1.dtype)\n",
    "print('   {:,.2f} Mb'.format(img1.nbytes * 0.000001))\n",
    "print('Image cropped =========')\n",
    "print(' ', img2.shape, ',', np.max(img2), ',', img2.dtype)\n",
    "print('   {:,.2f} Mb'.format(img2.size * img2.itemsize * 0.000001))\n",
    "print()\n",
    "\n",
    "# overlay ref image & cropped ref image\n",
    "for img_name in img_names:\n",
    "    for i in range(16):\n",
    "        # target image\n",
    "        img1 = img_refs_undistorted[img_name][i]\n",
    "        img2 = np.zeros((img1.shape[0], img1.shape[1]))\n",
    "        cx = img2.shape[0] // 2\n",
    "        cy = img2.shape[1] // 2\n",
    "        imgref = cv2.flip(cv2.resize(img_refs[img_name][i], (img1.shape[0], img1.shape[0])), -1)\n",
    "        w = imgref.shape[0] // 2\n",
    "        h = imgref.shape[0] // 2 \n",
    "        img2[cx-w:cx+w,cy-h:cy+h] = imgref\n",
    "        \n",
    "        img3 = np.zeros((img1.shape[0], img1.shape[1]))\n",
    "        cp = cv2.flip(cv2.resize(clean_plates[i], (img1.shape[0], img1.shape[0])), -1)\n",
    "        img3[cx-w:cx+w,cy-h:cy+h] = cp\n",
    "        \n",
    "        plt.figure(figsize=(15, 15))\n",
    "        plt.imshow(img1, cmap='gray', alpha=0.5)\n",
    "        plt.imshow(img2, cmap='gray', alpha=0.5)\n",
    "        break\n",
    "\n",
    "for i in range(16):\n",
    "    # clean plates\n",
    "    img1 = clean_plates_undistort[i]\n",
    "    img2 = clean_plates[i]\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(50, 50))\n",
    "    ax[0].imshow(img1, cmap='gray')\n",
    "    ax[0].set_title('undistorted')\n",
    "    ax[1].imshow(img2, cmap='gray')\n",
    "    ax[1].set_title('undistorted & cropped')\n",
    "    ax[1].invert_yaxis()\n",
    "    ax[1].invert_xaxis()\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = None\n",
    "torch.cuda.empty_cache()\n",
    "batch_size = 2\n",
    "n_batch = n_forwards // batch_size\n",
    "#$texturemap_path = r'./14g_data/output/200528/texturemap_averaged.npy'\n",
    "texturemap_path = r'./14h_data/output/200604/texturemap_averaged.npy'\n",
    "# texturemap_path = None\n",
    "model = Model_normal(device, texturemap_path=texturemap_path, texturemap_shape=texturemap_shape, img_dir=img_dir, cam_params=cams_torch, image_refs=img_refs, mesh_paths=mesh_paths, image_size=image_size, clean_plates=clean_plates, batch_size=batch_size, n_batch=n_batch)\n",
    "texture_maps = model.texture_map\n",
    "print('texture_map: {:,.2f}=={:,.2f} Mb'.format(texture_maps.element_size() * texture_maps.nelement() * 0.000001, texture_maps.detach().cpu().numpy().nbytes*0.000001))\n",
    "texture_maps_np = texture_maps.detach().cpu().numpy()\n",
    "print('  {} {:,.2f}Mb {} {}'.format(texture_maps.shape, texture_maps_np.nbytes*0.000001, texture_maps.dtype, np.max(texture_maps_np)))\n",
    "img_name = list(model.image_refs.keys())[0]\n",
    "img = model.image_refs[img_name][0]\n",
    "print('ref image: {}, {:,.2f} Mb'.format(img.shape, img.element_size() * img.nelement() * 0.000001))\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.imshow(texture_maps_np.squeeze())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "model.deform_meshes()\n",
    "# model.set_mesh_texturemap()\n",
    "images_with_bg = np.empty((n_batch*batch_size, model.image_size, model.image_size))\n",
    "images = np.empty((n_batch*batch_size, model.image_size, model.image_size, 4))\n",
    "images_target = np.empty((n_batch*batch_size, model.image_size, model.image_size))\n",
    "image_tensors = {}\n",
    "losses = []\n",
    "print('{} renders ({} images, {} cams), n_batch={}, batch_size={}'.format(n_forwards, len(img_names), len(cams), n_batch, batch_size))\n",
    "for batch_idx in range(n_batch):\n",
    "    print(batch_idx)\n",
    "    i0 = batch_idx*batch_size\n",
    "    i1 = i0 + batch_size\n",
    "    \n",
    "    imgs, l, l_dict = model(batch_idx, learn_texturemap=False, learn_deform=False)\n",
    "    losses.append(l.detach().cpu().data)\n",
    "    for k, img in imgs.items():\n",
    "        if k in image_tensors:\n",
    "            image_tensors[k].append(img)\n",
    "        else:\n",
    "            image_tensors[k] = [img]\n",
    "            \n",
    "    images_with_bg[i0:i1] = imgs['currents_with_bg'].detach().cpu().numpy()\n",
    "    images[i0:i1] = imgs['currents'].detach().cpu().numpy()\n",
    "    images_target[i0:i1] = imgs['targets'].detach().cpu().numpy()\n",
    "    # model.save_parameters('./7_data/output/deform_verts.npy')\n",
    "    # model.export_obj('./7_data/output/obj.obj')\n",
    "    if batch_idx > -1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for img_pair in img_pairs:\n",
    "    img_cur = img_pair[0].squeeze().cpu().numpy()\n",
    "    img_ref = img_pair[1].squeeze().cpu().numpy()\n",
    "    \n",
    "    for batch_idx in range(img_cur.shape[0]):\n",
    "        img1 = cv2.flip(img_cur[batch_idx], -1)\n",
    "        img2 = cv2.flip(img_ref[batch_idx], -1)\n",
    "        diff = np.abs(img1-img2)\n",
    "        fig, ax = plt.subplots(1, 3, figsize=(10, 5))\n",
    "        ax[0].imshow(img1, cmap='gray')\n",
    "        ax[0].set_title('Current {}'.format(img1.shape))\n",
    "        ax[1].imshow(img2, cmap='gray')\n",
    "        ax[1].set_title('Target {}'.format(img2.shape))\n",
    "        ax[2].imshow(np.abs(img1-img2))\n",
    "        ax[2].set_title('Difference\\n{}|({:.2f}, {:.2f})'.format(diff.shape, np.min(diff), np.max(diff)))\n",
    "        plt.show()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for img_idx in range(len(img_names)):\n",
    "    for cam_idx in range(len(cams)):\n",
    "        i = img_idx*len(cams) + cam_idx\n",
    "        fig, ax = plt.subplots(1, 4, figsize=(30, 10))\n",
    "        ax = ax.ravel()\n",
    "        img = cv2.flip(images[i], -1)\n",
    "        ax[0].imshow(img[..., 3])\n",
    "        ax[0].set_title('{}. {}.pgm, cam[{}/{}]'.format(i, img_names[img_idx], cam_idx+1, len(cams)))\n",
    "        ax[0].set_xlabel('{}, [{}, {}]'.format(img.shape, np.min(img), np.max(img)))\n",
    "        \n",
    "        ax[1].imshow(img)\n",
    "        ax[1].set_xlabel('{}, [{}, {}]'.format(img.shape, np.min(img), np.max(img)))\n",
    "        \n",
    "        img2 = cv2.flip(images_target[i], -1)\n",
    "        ax[2].imshow(img2, cmap='gray')\n",
    "        ax[2].set_xlabel('{}, [{}, {}]'.format(img2.shape, np.min(img2), np.max(img2)))\n",
    "        \n",
    "        diff = np.abs(img2 - np.mean(img, axis=-1))\n",
    "        ax[3].imshow(diff, cmap='gray')\n",
    "        \n",
    "        plt.show()\n",
    "        break\n",
    "    break\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RoG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from laplacian_pyramid import *\n",
    "\n",
    "channels = 1\n",
    "kernel_size = 5\n",
    "alpha = 0.45\n",
    "lap_pyr = LaplacianPyramid(device, image_size, channels, kernel_size, alpha)\n",
    "\n",
    "image_input = image_tensors['currents_with_bg'][i].unsqueeze(-1)\n",
    "print('image_input:',image_input.shape)\n",
    "laplacians, image_final = lap_pyr.build_laplacian_pyramid(image_input, n_res=3)\n",
    "image_reconst = lap_pyr.reconstruct_original_images(image_final, laplacians)\n",
    "\n",
    "image_input = cv2.flip(image_input[0].detach().squeeze().cpu().numpy(), -1)\n",
    "image_reconst = cv2.flip(image_reconst[0].detach().squeeze().cpu().numpy(), -1)\n",
    "diff = np.abs(image_input - image_reconst)\n",
    "\n",
    "img = diff\n",
    "print(np.sum(diff))\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.title('{} | ({:.2f}, {:.2f})'.format(img.shape, np.min(img), np.max(img)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_lap = len(laplacians)\n",
    "for band in range(n_lap):\n",
    "    laplacian = laplacians[band]\n",
    "    for batch_idx in range(laplacian.shape[0]):\n",
    "        lap = laplacian[batch_idx].detach().squeeze().cpu().numpy()\n",
    "        lap = cv2.flip(lap, -1)\n",
    "        #s = int(lap.shape[1] / 50)\n",
    "        s = 10\n",
    "        plt.figure(figsize=(s, s))\n",
    "        plt.imshow(lap, cmap='gray')\n",
    "        plt.title('level={}, batch_idx={} | {}'.format(band, batch_idx, lap.shape))\n",
    "        plt.show()\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "class GaussianLayer(nn.Module):\n",
    "    def __init__(self, device, channels, kernel_size, sigma):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        \n",
    "        self.kernel_size = kernel_size\n",
    "        self.sigma = sigma\n",
    "        self.channels = channels\n",
    "\n",
    "        self.conv = self._get_gaussian_conv2d(kernel_size, sigma, channels)\n",
    "        gaussian_kernel = self._generate_gaussian_kernel(kernel_size=kernel_size, sigma=sigma, channels=channels)\n",
    "        self.conv.weight.data = gaussian_kernel\n",
    "        self.conv.padding = int(kernel_size/2)\n",
    "        self.downsample_conv2d = self._generate_downsampler_conv2d(kernel_size=kernel_size, channels=channels)\n",
    "     \n",
    "    def forward(self, x, kernel_size, sigma, channels=4):\n",
    "        \"\"\"\n",
    "        input: x=[N, C, W, H], not [N, W, H, C]\n",
    "        \"\"\"\n",
    "        out = self.conv(x)\n",
    "        out = out.permute(0, 2, 3, 1)\n",
    "        return out\n",
    "    \n",
    "    def downsample(self, img):\n",
    "        img = self.downsample_conv2d(img)\n",
    "        return img.permute(0, 2, 3, 1)\n",
    "    \n",
    "    def _generate_gaussian_kernel(self, kernel_size, sigma, channels=4):\n",
    "        # Create a x, y coordinate grid of shape (kernel_size, kernel_size, 2)\n",
    "        x_cord = torch.arange(kernel_size)\n",
    "        x_grid = x_cord.repeat(kernel_size).view(kernel_size, kernel_size)\n",
    "        y_grid = x_grid.t()\n",
    "        xy_grid = torch.stack([x_grid, y_grid], dim=-1)\n",
    "\n",
    "        mean = (kernel_size - 1)/2.\n",
    "        variance = sigma**2.\n",
    "\n",
    "        # Calculate the 2-dimensional gaussian kernel which is\n",
    "        # the product of two gaussian distributions for two different\n",
    "        # variables (in this case called x and y)\n",
    "        gaussian_kernel = (1./(2.*math.pi*variance)) * torch.exp(-torch.sum((xy_grid - mean)**2., dim=-1) / (2*variance + 0.00001))\n",
    "        \n",
    "#         gaussian_kernel = torch.sum((xy_grid - mean)**2, dim=-1)\n",
    "        # Make sure sum of values in gaussian kernel equals 1.\n",
    "        gaussian_kernel = gaussian_kernel / torch.sum(gaussian_kernel)\n",
    "        \n",
    "        # Reshape to 2d depthwise convolutional weight\n",
    "        gaussian_kernel = gaussian_kernel.view(1, 1, kernel_size, kernel_size)\n",
    "        gaussian_kernel = gaussian_kernel.repeat(channels, 1, 1, 1).to(self.device)\n",
    "        return gaussian_kernel\n",
    "    \n",
    "    def _get_gaussian_conv2d(self, kernel_size, sigma, channels=4):\n",
    "        gaussian_filter = nn.Conv2d(in_channels=channels, out_channels=channels, kernel_size=kernel_size, stride=1, padding=int(kernel_size/2), groups=channels, bias=False).to(self.device)\n",
    "        gaussian_kernel = self._generate_gaussian_kernel(kernel_size=kernel_size, sigma=sigma, channels=channels)\n",
    "        gaussian_filter.weight.data = gaussian_kernel\n",
    "        print('gaussian_kernel:', gaussian_kernel.shape)\n",
    "        gaussian_filter.weight.requires_grad = False\n",
    "        return gaussian_filter\n",
    "\n",
    "    def _generate_downsampler_conv2d(self, kernel_size, channels=4):\n",
    "        kernel_data = torch.ones((kernel_size, kernel_size)) / kernel_size**2\n",
    "        kernel_data = kernel_data.view(1, 1, kernel_size, kernel_size)\n",
    "        kernel_data = kernel_data.repeat(1, channels, 1, 1).to(self.device)\n",
    "        \n",
    "        kernel = nn.Conv2d(in_channels=channels, out_channels=channels, kernel_size=kernel_size, stride=2, padding=int(kernel_size/2), groups=channels, bias=False).to(self.device)\n",
    "        kernel.weight.data = kernel_data\n",
    "        kernel.weight.requires_grad = False\n",
    "        return kernel\n",
    "\n",
    "i = 0\n",
    "gaussian = GaussianLayer(model.device, kernel_size=3, sigma=1, channels=1)\n",
    "\n",
    "# =============================== #\n",
    "# RoG\n",
    "# =============================== #\n",
    "rogs = {'originals': [], 'gaussians': [], 'laplacians': []}\n",
    "img_gauss_last = None\n",
    "for res in range(0, 4):\n",
    "    if res == 0:\n",
    "        img = image_tensors['currents_with_bg'][i].unsqueeze(-1)\n",
    "    else:\n",
    "        img = gaussian.downsample(img_gauss_last.permute(0, 3, 1, 2))\n",
    "    img_gauss = gaussian(img.permute(0, 3, 1, 2), kernel_size=3, sigma=1, channels=1)\n",
    "    L = img - img_gauss\n",
    "    \n",
    "    rogs['originals'].append(img)\n",
    "    rogs['gaussians'].append(img_gauss)\n",
    "    rogs['laplacians'].append(L)\n",
    "\n",
    "    img_gauss_last = img_gauss.clone()\n",
    "    \n",
    "batch_idx = 0\n",
    "img_gauss_last = None\n",
    "for res in range(0, 4):\n",
    "    img_original = rogs['originals'][res][batch_idx].detach().squeeze().cpu().numpy()\n",
    "    img_original = cv2.flip(img_original, -1)\n",
    "\n",
    "    img_gauss = rogs['gaussians'][res][batch_idx].detach().squeeze().cpu().numpy()\n",
    "    img_gauss = cv2.flip(img_gauss, -1)\n",
    "\n",
    "    img_lap = rogs['laplacians'][res][batch_idx].detach().squeeze().cpu().numpy()\n",
    "    img_lap = cv2.flip(img_lap, -1)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 4, figsize=(32, 8))\n",
    "\n",
    "    if img_gauss_last is not None:\n",
    "        dsize = (int(img_gauss_last.shape[0]/2), int(img_gauss_last.shape[1]/2))\n",
    "        img_gauss_last = cv2.resize(img_gauss_last, dsize=dsize, interpolation=cv2.INTER_AREA)\n",
    "        \n",
    "        sanity = img_gauss_last - img_original\n",
    "        ax[0].imshow(sanity, cmap='gray')\n",
    "        ax[0].set_title('sanity check | ({:.2f}, {:.2f})'.format(np.min(sanity), np.max(sanity)))\n",
    "    else:\n",
    "        ax[0].axis(False)\n",
    "\n",
    "    ax[1].imshow(img_original, cmap='gray')\n",
    "    ax[2].imshow(img_gauss, cmap='gray')\n",
    "    ax[2].set_title('Guassian blurred | {}'.format(img_gauss.shape))\n",
    "\n",
    "    ax[3].imshow(img_lap, cmap='gray')\n",
    "    ax[3].set_title('Difference | [{}] {}'.format(res, img_lap.shape))\n",
    "    plt.show()\n",
    "        \n",
    "    img_gauss_last = img_gauss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reprojection check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_points = {}\n",
    "\n",
    "t0 = time.time()\n",
    "for i in range(len(model.meshes)):\n",
    "    mesh_vertices = model.meshes[i].verts_packed().cpu()\n",
    "    \n",
    "    pts = []\n",
    "    for cam_idx in range(16):\n",
    "        params = cam_params[cam_idx]\n",
    "        p = reproject(params, mesh_vertices, distort=False)\n",
    "        pts.append(p)\n",
    "    \n",
    "    img_name = img_names[i]\n",
    "    mesh_points[img_name] = pts\n",
    "t1 = time.time()\n",
    "print('{:.2f}s'.format(t1-t0))\n",
    "print(len(mesh_points))\n",
    "del mesh_vertices, pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in mesh_points.items():\n",
    "    print(k, ':', len(v), 'cameras,', v[0].shape, 'points/camera')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "bd = model.batch_dict\n",
    "mesh_idx = bd['mesh_idx']\n",
    "\n",
    "for mesh_idx, img_name in enumerate(list(mesh_points.keys())):\n",
    "    print(img_name)\n",
    "    \n",
    "    for cam_idx in range(model.n_cams):\n",
    "        i = mesh_idx*model.n_cams + cam_idx\n",
    "        \n",
    "        plt.figure(figsize=(10, 10))\n",
    "        \n",
    "        # bg\n",
    "        img_bg = img_refs[img_name][cam_idx]\n",
    "        img_bg = cv2.flip(img_bg, -1)\n",
    "        plt.imshow(img_bg, alpha=0.5, cmap='gray')\n",
    "\n",
    "        # mesh from pytorch3d\n",
    "        img_mesh = images[i]\n",
    "        img_mesh = cv2.flip(img_mesh, -1)\n",
    "        plt.imshow(img_mesh, alpha=0.5, cmap='gray')\n",
    "\n",
    "        pts = mesh_points[img_name][cam_idx]\n",
    "        pts_small_x = (pts[:, 0] - (4000-2160)*0.5) * image_size/2160\n",
    "        pts_small_y = pts[:, 1] * image_size/2160\n",
    "        pts_small = np.stack([pts_small_x, pts_small_y]).T\n",
    "        pts_center = np.mean(pts_small, axis=0)\n",
    "        plt.scatter(pts_small[:, 0], pts_small[:, 1], c='b', s=0.1)\n",
    "        plt.title('Camera {}'.format(i))\n",
    "\n",
    "        # plot centers\n",
    "        plt.scatter(pts_center[0], pts_center[1], c='r')\n",
    "\n",
    "        if i == 0:\n",
    "            print('reference image:', np.max(img_bg), img_bg.dtype, img_bg.shape)\n",
    "            print('rendered pytorch image :', np.max(img_mesh), img_mesh.dtype, img_mesh.shape, ', {:,.2f} Mb'.format(img_mesh.nbytes * 0.000001), np.max(img_mesh))\n",
    "        plt.show()\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# before training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dverts(out_dir, e, batch_idx, lr, img_name, cam_idx, loss_dict, grads, image_curr, image_target, img_sil=None):\n",
    "    fig, ax = plt.subplots(1, 6, figsize=(30, 6), tight_layout=True)\n",
    "    plt.suptitle('Epoch {} | {}.pgm | lr={}'.format(e, img_name, lr), fontsize=16)\n",
    "    a_loss = ax[0]\n",
    "    a_grad = ax[1]\n",
    "    a_curr = ax[2]\n",
    "    a_target = ax[3]\n",
    "    a_overlay = ax[4]\n",
    "    a_diff = ax[5]\n",
    "    \n",
    "    legend_str = ['total']\n",
    "    losses = loss_dict['total']\n",
    "    a_loss.plot(losses, linewidth=3)\n",
    "    a_loss.set_title('losses: {}'.format(losses[-1]))\n",
    "    for k, l in loss_dict.items():\n",
    "        if k != 'total':\n",
    "            a_loss.plot(l)\n",
    "            legend_str.append(k)\n",
    "    a_loss.legend(legend_str)\n",
    "    a_loss.grid()\n",
    "    \n",
    "    a_grad.set_title('||grad||: {}'.format(grads[-1]))\n",
    "    a_grad.plot(grads)\n",
    "    a_grad.grid()\n",
    "    \n",
    "    a_curr.imshow(image_curr, cmap='gray', vmin=0, vmax=1.0)\n",
    "    a_curr.set_title('current | camera {}'.format(cam_idx))\n",
    "    a_curr.set_xlabel('{}'.format(image_curr.shape))\n",
    "    \n",
    "    a_target.imshow(image_target, cmap='gray', vmin=0, vmax=1.0)\n",
    "    if img_sil is not None:\n",
    "        a_target.imshow(img_sil, alpha=0.25)\n",
    "    a_target.set_title('target')\n",
    "    a_target.set_xlabel('{}'.format(image_target.shape))\n",
    "    \n",
    "    a_overlay.imshow(image_target, cmap='gray', vmin=0, vmax=1.0)\n",
    "    a_overlay.imshow(image_curr, alpha=0.5, cmap='gray', vmin=0, vmax=1.0)\n",
    "    a_overlay.set_title('overlay')\n",
    "    \n",
    "    diff = image_curr - image_target\n",
    "    diff_abs = np.abs(diff)\n",
    "    a_diff.imshow(diff_abs)\n",
    "    a_diff.set_title('difference')\n",
    "    a_diff.set_xlabel('{}, min={:.2f}, max={:.2f}'.format(diff.shape, np.min(diff), np.max(diff)))\n",
    "    \n",
    "    out_path = out_dir + '/plot_{}.png'.format(e)\n",
    "    plt.savefig(out_path, dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    plot_img = imageio.imread(out_path)\n",
    "    return plot_img\n",
    "\n",
    "def plot_dverts_initial_final_2x2(out_dir, i, N, img_name, cam_idx, image_target, image_curr1, image_curr2, image_sil1, image_sil2):\n",
    "    fig, ax = plt.subplots(2, 2, figsize=(12, 13), tight_layout=True)\n",
    "    plt.suptitle('[{}/{}] {}.pgm'.format(i+1, N, e, img_name), fontsize=16)\n",
    "\n",
    "    for c in range(2):\n",
    "        if c == 0:\n",
    "            prefix_str = '[Initial]'\n",
    "            image_curr = image_curr1\n",
    "            image_sil = image_sil1\n",
    "        else:\n",
    "            prefix_str = '[Final]'\n",
    "            image_curr = image_curr2\n",
    "            image_sil = image_sil2\n",
    "            \n",
    "        a_target = ax[0, c]\n",
    "        a_overlay = ax[1, c]\n",
    "\n",
    "        a_target.imshow(image_target, cmap='gray', vmin=0, vmax=1.0)\n",
    "        if img_sil is not None:\n",
    "            a_target.imshow(img_sil, alpha=0.25)\n",
    "        a_target.set_title('{} target'.format(prefix_str))\n",
    "        a_target.set_xlabel('{}'.format(image_target.shape))\n",
    "\n",
    "        a_overlay.imshow(image_target, cmap='gray', vmin=0, vmax=1.0)\n",
    "        a_overlay.imshow(image_curr, alpha=0.5, cmap='gray', vmin=0, vmax=1.0)\n",
    "        a_overlay.set_title('{} overlay'.format(prefix_str))\n",
    "\n",
    "    out_path = out_dir + '/plot_final_{}.png'.format(i)\n",
    "    plt.savefig(out_path, dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    plot_img = imageio.imread(out_path)\n",
    "    return plot_img\n",
    "\n",
    "def plot_dverts_initial_final(out_dir, i, N, img_name, cam_idx, image_target, image_curr1, image_curr2, image_sil1, image_sil2):\n",
    "    fig, ax = plt.subplots(2, 4, figsize=(24, 13), tight_layout=True)\n",
    "    plt.suptitle('[{}/{}] {}.pgm'.format(i+1, N, e, img_name), fontsize=16)\n",
    "\n",
    "    for r in range(2):\n",
    "        if r == 0:\n",
    "            prefix_str = 'initial'\n",
    "            image_curr = image_curr1\n",
    "            image_sil = image_sil1\n",
    "        else:\n",
    "            prefix_str = 'final'\n",
    "            image_curr = image_curr2\n",
    "            image_sil = image_sil2\n",
    "            \n",
    "        a_curr = ax[r, 0]\n",
    "        a_target = ax[r, 1]\n",
    "        a_overlay = ax[r, 2]\n",
    "        a_diff = ax[r, 3]\n",
    "\n",
    "        a_curr.imshow(image_curr, cmap='gray', vmin=0, vmax=1.0)\n",
    "        a_curr.set_title('{} current | camera {}'.format(prefix_str, cam_idx))\n",
    "        a_curr.set_xlabel('{}'.format(image_curr.shape))\n",
    "\n",
    "        a_target.imshow(image_target, cmap='gray', vmin=0, vmax=1.0)\n",
    "        if image_sil is not None:\n",
    "            a_target.imshow(image_sil, alpha=0.25)\n",
    "        a_target.set_title('{} target'.format(prefix_str))\n",
    "        a_target.set_xlabel('{}'.format(image_target.shape))\n",
    "\n",
    "        a_overlay.imshow(image_target, cmap='gray', vmin=0, vmax=1.0)\n",
    "        a_overlay.imshow(image_curr, alpha=0.5, cmap='gray', vmin=0, vmax=1.0)\n",
    "        a_overlay.set_title('{} overlay'.format(prefix_str))\n",
    "\n",
    "        diff = image_curr - image_target\n",
    "        diff_abs = np.abs(diff)\n",
    "        a_diff.imshow(diff_abs)\n",
    "        a_diff.set_title('{} difference'.format(prefix_str))\n",
    "        a_diff.set_xlabel('{}, min={:.2f}, max={:.2f}'.format(diff.shape, np.min(diff), np.max(diff)))\n",
    "\n",
    "    out_path = out_dir + '/plot_final_{}.png'.format(i)\n",
    "    plt.savefig(out_path, dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    plot_img = imageio.imread(out_path)\n",
    "    return plot_img\n",
    "\n",
    "def plot_3_dverts(out_dir, prefix_str, i, N, img_name, cam_idx, image_target, image_curr, image_sil):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(20, 11), tight_layout=True)\n",
    "    plt.suptitle('[{}/{}] {}.pgm'.format(i+1, N, img_name), fontsize=16)\n",
    "\n",
    "    a_target = ax[0]\n",
    "    a_overlay = ax[1]\n",
    "\n",
    "    a_target.imshow(image_target, cmap='gray', vmin=0, vmax=1.0)\n",
    "    if image_sil is not None:\n",
    "        a_target.imshow(image_sil, alpha=0.25)\n",
    "    a_target.set_title('{} target'.format(prefix_str))\n",
    "    a_target.set_xlabel('{}'.format(image_target.shape))\n",
    "\n",
    "    a_overlay.imshow(image_target, cmap='gray', vmin=0, vmax=1.0)\n",
    "    a_overlay.imshow(image_curr, alpha=0.5, cmap='gray', vmin=0, vmax=1.0)\n",
    "    a_overlay.set_title('{} overlay'.format(prefix_str))\n",
    "\n",
    "    out_path = out_dir + '/compare_{}_{}.png'.format(i, prefix_str)\n",
    "    plt.savefig(out_path, dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    plot_img = imageio.imread(out_path)\n",
    "    return plot_img\n",
    "\n",
    "def plot_6(out_dir, e, batch_idx, lr, img_name, cam_idx, loss_dict, grads, image_curr, image_target, texturemap, img_sil=None):\n",
    "    fig, ax = plt.subplots(1, 6, figsize=(30, 6), tight_layout=True)\n",
    "    plt.suptitle('Epoch {} | {}.pgm | lr={}'.format(e, img_name, lr), fontsize=16)\n",
    "    a_loss = ax[0]\n",
    "    a_grad = ax[1]\n",
    "    a_curr = ax[2]\n",
    "    a_target = ax[3]\n",
    "    a_diff = ax[4]\n",
    "    a_tex = ax[5]\n",
    "    \n",
    "    legend_str = ['total']\n",
    "    losses = loss_dict['total']\n",
    "    a_loss.plot(losses)\n",
    "    a_loss.set_title('losses: {}'.format(losses[-1]))\n",
    "    for k, l in loss_dict.keys():\n",
    "        if k != 'total':\n",
    "            a_loss.plot(l)\n",
    "            legend_str.append(k)\n",
    "    a_loss.legend(legend_str)\n",
    "    a_loss.grid()\n",
    "    \n",
    "    a_grad.set_title('||grad||: {}'.format(grads[-1]))\n",
    "    a_grad.plot(grads)\n",
    "    a_grad.grid()\n",
    "    \n",
    "    a_curr.imshow(image_curr, cmap='gray', vmin=0, vmax=1.0)\n",
    "    a_curr.set_title('current | camera {}'.format(cam_idx))\n",
    "    a_curr.set_xlabel('{}'.format(image_curr.shape))\n",
    "    \n",
    "    a_target.imshow(image_target, cmap='gray', vmin=0, vmax=1.0)\n",
    "    if img_sil is not None:\n",
    "        a_target.imshow(img_sil, alpha=0.25)\n",
    "    a_target.set_title('target')\n",
    "    a_target.set_xlabel('{}'.format(image_target.shape))\n",
    "    \n",
    "    diff = image_curr - image_target\n",
    "    diff_abs = np.abs(diff)\n",
    "    a_diff.imshow(diff_abs)\n",
    "    a_diff.set_title('difference')\n",
    "    a_diff.set_xlabel('{}, min={:.2f}, max={:.2f}'.format(diff.shape, np.min(diff), np.max(diff)))\n",
    "    \n",
    "    a_tex.imshow(np.clip(texturemap, a_min=0, a_max=1.0), cmap='gray', vmin=0, vmax=1.0)\n",
    "    a_tex.set_title('texturemap')\n",
    "    a_tex.set_xlabel('{}, min={:.2f}, max={:.2f}'.format(texturemap.shape, np.min(texturemap), np.max(texturemap)))\n",
    "    \n",
    "    out_path = out_dir + '/plot_{}.png'.format(e)\n",
    "    plt.savefig(out_path, dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    plot_img = imageio.imread(out_path)\n",
    "    return plot_img\n",
    "\n",
    "\n",
    "def plot_3(out_dir, i, N, img_name, cam_idx, image_curr, image_target):\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(18, 7), tight_layout=True)\n",
    "    plt.suptitle('[{}/{}] {}.pgm'.format(i+1, N, e, img_name), fontsize=16)\n",
    "    a_curr = ax[0]\n",
    "    a_target = ax[1]\n",
    "    a_diff = ax[2]\n",
    "    \n",
    "    a_curr.imshow(image_curr, cmap='gray', vmin=0, vmax=1.0)\n",
    "    a_curr.set_title('current | camera {}'.format(cam_idx))\n",
    "    a_curr.set_xlabel('{}'.format(image_curr.shape))\n",
    "    \n",
    "    a_target.imshow(image_target, cmap='gray', vmin=0, vmax=1.0)\n",
    "    a_target.set_title('target')\n",
    "    a_target.set_xlabel('{}'.format(image_target.shape))\n",
    "    \n",
    "    diff = image_curr - image_target\n",
    "    diff_abs = np.abs(diff)\n",
    "    a_diff.imshow(diff_abs)\n",
    "    a_diff.set_title('difference')\n",
    "    a_diff.set_xlabel('{}, min={:.2f}, max={:.2f}'.format(diff.shape, np.min(diff), np.max(diff)))\n",
    "    \n",
    "    out_path = out_dir + '/plot_final_{}.png'.format(i)\n",
    "    plt.savefig(out_path, dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    plot_img = imageio.imread(out_path)\n",
    "    return plot_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test forward, backward & plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "texturemap_path = r'./14g_data/output/200528/texturemap_averaged.npy'\n",
    "# texturemap_path = r'D:\\1_Projects\\200325_PyTorch3d_Toy\\9_data\\output\\200413_SingleVsMulti\\multi_L1\\texturemap_learned.png'\n",
    "if texturemap_path is not None:\n",
    "    if '.png' in texturemap_path:\n",
    "        texturemap = imageio.imread(texturemap_path) / 255.0\n",
    "        print(texturemap.shape)\n",
    "    elif '.npy' in texturemap_path:\n",
    "        texturemap = np.load(texturemap_path).astype(np.float32)\n",
    "    texture_map_torch = torch.from_numpy(texturemap).unsqueeze(0).unsqueeze(-1).float()\n",
    "else:\n",
    "    texture_map_torch = torch.from_numpy(np.ones((1, texturemap_shape[0], texturemap_shape[1], 1)).astype(np.float32))\n",
    "print(texture_map_torch.shape, texture_map_torch.dtype, torch.min(texture_map_torch), torch.max(texture_map_torch))\n",
    "\n",
    "texture_map = texture_map_torch.detach().squeeze().cpu().numpy()\n",
    "print(texture_map.shape, texture_map.dtype, np.min(texture_map), np.max(texture_map))\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(texture_map, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# =================================================================================== #\n",
    "batch_size = 2\n",
    "n_batch = n_forwards // batch_size\n",
    "texturemap_path = r'./14g_data/output/200528/texturemap_averaged.npy'\n",
    "# texturemap_path = r'D:\\1_Projects\\200325_PyTorch3d_Toy\\9_data\\output\\200413_SingleVsMulti\\multi_L1\\texturemap_learned.png'\n",
    "model = Model_normal(device, texturemap_path=texturemap_path, texturemap_shape=texturemap_shape, img_dir=img_dir, cam_params=cams_torch, image_refs=img_refs, mesh_paths=mesh_paths, image_size=image_size, clean_plates=clean_plates, batch_size=batch_size, n_batch=n_batch)\n",
    "\n",
    "print('----- torch.cuda.empty_cache() -----')\n",
    "torch.cuda.empty_cache()\n",
    "bytes_alloc = torch.cuda.memory_allocated()\n",
    "print('torch.cuda.memory_allocated(): {:,.2f} Mb'.format(bytes_alloc * 0.000001))\n",
    "bytes_cached = torch.cuda.memory_cached()\n",
    "print('torch.cuda.memory_cached(): {:,.2f} Mb'.format(bytes_cached * 0.000001))\n",
    "# =================================================================================== #\n",
    "\n",
    "out_dir = './16f_data/output'\n",
    "# vt_path = mesh_dir + '/{}Interpo_mm.obj'.format(img_names[0])\n",
    "lr = 1.0\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "model.deform_meshes()\n",
    "\n",
    "print('{} renders ({} images, {} cams), n_batch={}, batch_size={}'.format(n_forwards, len(img_names), len(cams), n_batch, batch_size))\n",
    "plot_idx = 0\n",
    "e = 0\n",
    "losses = {'total': [0, 0.1], 'pixel': [0, 0.2], 'laplacian': [0.3, 0.1]}\n",
    "grad_norms = [0, 0.1]\n",
    "for batch_idx in range(2, n_batch):\n",
    "    t0 = time.time()\n",
    "    i0 = batch_idx*batch_size\n",
    "    i1 = i0 + batch_size\n",
    "    img_name = model.batch_dict['img_name'][i0:i1][plot_idx]\n",
    "    cam_idx = model.batch_dict['cam_idx'][i0:i1][plot_idx]\n",
    "    \n",
    "    # ================== #\n",
    "    # train\n",
    "    # ================== #\n",
    "    optimizer.zero_grad()\n",
    "    imgs, l, l_dict = model(batch_idx, learn_texturemap=False, learn_deform=True)\n",
    "    l.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "#     model.export_obj(out_dir, vt_path=vt_path, fname_suffix='')\n",
    "    \n",
    "    img_sil = cv2.flip(imgs['currents'][plot_idx][..., 3].detach().squeeze().cpu().numpy(), -1)\n",
    "    img_sil = np.dstack([np.where(img_sil > 0, 1, 0), np.zeros(img_sil.shape), np.zeros(img_sil.shape)])\n",
    "    img_curr = cv2.flip(imgs['currents_with_bg'][plot_idx].detach().squeeze().cpu().numpy(), -1)\n",
    "    img_target = cv2.flip(imgs['targets'][plot_idx].detach().squeeze().cpu().numpy(), -1)\n",
    "    \n",
    "    losses['total'].append(l_dict['total'].detach().cpu().numpy())\n",
    "    losses['pixel'].append(l_dict['pixel'].detach().cpu().numpy())\n",
    "    losses['laplacian'].append(l_dict['laplacian'].detach().cpu().numpy())\n",
    "    grad_value = model.dnormals.grad.detach().cpu().numpy()\n",
    "    grad_norms.append(np.linalg.norm(grad_value))\n",
    "    texturemap = model.texture_map.clone().detach().squeeze().cpu().numpy()\n",
    "    \n",
    "    t1 = time.time()\n",
    "    plot_img = plot_dverts(out_dir, e, batch_idx, 0.001, img_name, cam_idx, losses, grad_norms, img_curr, img_target, img_sil)\n",
    "    plt.figure(figsize=(30, 6))\n",
    "    plt.imshow(plot_img)\n",
    "    plt.xticks([]), plt.yticks([])\n",
    "    t2 = time.time()\n",
    "    print('run: {:.2f}s, plot: {:.2f}s'.format(t1-t0, t2-t1))\n",
    "    \n",
    "    plt.show()\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# === Train ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_settings_str():\n",
    "    log_str = ''\n",
    "    \n",
    "    log_str += '{:<15}: {}\\n'.format('cameras', cam_path)\n",
    "    log_str += '{:<15}: {}\\n'.format('mesh', mesh_dir)\n",
    "    log_str += '{:<15}: {}\\n'.format('clean plates', clean_plate_dir)\n",
    "    log_str += '{:<15}:'.format('images')\n",
    "    for img_name in img_names:\n",
    "        log_str += ' {}.pgm'.format(img_name)\n",
    "    log_str += '\\n'\n",
    "    log_str += '---------------------------------------------\\n\\n'\n",
    "    return log_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initial renderings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "images_alpha_initial = np.empty((model.n_batch*model.batch_size, model.image_size, model.image_size))\n",
    "images_initial = np.empty((model.n_batch*model.batch_size, model.image_size, model.image_size))\n",
    "targets = np.empty((model.n_batch*model.batch_size, model.image_size, model.image_size))\n",
    "\n",
    "# =================================================================================== #\n",
    "batch_size = 1\n",
    "n_batch = n_forwards // batch_size\n",
    "# texturemap_path = r'D:\\1_Projects\\200325_PyTorch3d_Toy\\9_data\\output\\200413_SingleVsMulti\\multi_L1\\texturemap_learned.png'\n",
    "texturemap_path = r'./14h_data/output/200604/texturemap_averaged.npy'\n",
    "model = Model_normal(device, texturemap_path=texturemap_path, texturemap_shape=texturemap_shape, img_dir=img_dir, cam_params=cams_torch, image_refs=img_refs, mesh_paths=mesh_paths, image_size=image_size, clean_plates=clean_plates, batch_size=batch_size, n_batch=n_batch)\n",
    "\n",
    "print('----- torch.cuda.empty_cache() -----')\n",
    "torch.cuda.empty_cache()\n",
    "bytes_alloc = torch.cuda.memory_allocated()\n",
    "print('torch.cuda.memory_allocated(): {:,.2f} Mb'.format(bytes_alloc * 0.000001))\n",
    "bytes_cached = torch.cuda.memory_cached()\n",
    "print('torch.cuda.memory_cached(): {:,.2f} Mb'.format(bytes_cached * 0.000001))\n",
    "# =================================================================================== #\n",
    "\n",
    "model.deform_meshes()\n",
    "for batch_idx in range(model.n_batch):\n",
    "    print(' {}/{}'.format(batch_idx+1, model.n_batch), end='')\n",
    "    i0 = batch_idx*model.batch_size\n",
    "    i1 = i0 + model.batch_size\n",
    "    imgs, l, l_dict = model(batch_idx, learn_texturemap=False, learn_deform=False)\n",
    "\n",
    "    images_initial[i0:i1] = imgs['currents_with_bg'].squeeze().detach().cpu().numpy()\n",
    "    targets[i0:i1] = imgs['targets'].squeeze().detach().cpu().numpy()\n",
    "    images_alpha_initial[i0:i1] = imgs['currents'][..., 3].squeeze().detach().cpu().numpy()\n",
    "print('\\nDone')\n",
    "\n",
    "out_idx = 8\n",
    "out_dir = './16g_data/output'\n",
    "img_name = model.batch_dict['img_name'][out_idx]\n",
    "cam_idx = model.batch_dict['cam_idx'][out_idx]\n",
    "img_curr = cv2.flip(images_initial[out_idx], -1)\n",
    "img_sil = cv2.flip(images_alpha_initial[out_idx], -1)\n",
    "img_sil = np.dstack([np.where(img_sil > 0, 1, 0), np.zeros(img_sil.shape), np.zeros(img_sil.shape)])\n",
    "img_target = cv2.flip(targets[out_idx], -1)\n",
    "\n",
    "plot_img = plot_dverts(out_dir, '0init', batch_idx, 0.001, img_name, cam_idx, {'total': [0], 'pixel': [0], 'laplacian': [0]}, [0], img_curr, img_target, img_sil)\n",
    "plt.figure(figsize=(30, 6))\n",
    "plt.imshow(plot_img)\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# =================================================================================== #\n",
    "model = Model_normal(device, texturemap_path=texturemap_path, texturemap_shape=texturemap_shape, img_dir=img_dir, cam_params=cams_torch, image_refs=img_refs, mesh_paths=mesh_paths, image_size=image_size, clean_plates=clean_plates, batch_size=batch_size, n_batch=n_batch)\n",
    "print('----- torch.cuda.empty_cache() -----')\n",
    "torch.cuda.empty_cache()\n",
    "bytes_alloc = torch.cuda.memory_allocated()\n",
    "print('torch.cuda.memory_allocated(): {:,.2f} Mb'.format(bytes_alloc * 0.000001))\n",
    "bytes_cached = torch.cuda.memory_cached()\n",
    "print('torch.cuda.memory_cached(): {:,.2f} Mb'.format(bytes_cached * 0.000001))\n",
    "# =================================================================================== #\n",
    "vt_path = mesh_dir + '/{}.obj'.format(img_names[0])\n",
    "\n",
    "now = datetime.now()\n",
    "date_str = '{}{:>02}{:>02}'.format(now.year, now.month, now.day)\n",
    "log_path = out_dir + '/log_{}.txt'.format(date_str)\n",
    "__output_log(log_path, '=== {} Start ==========================\\n'.format(date_str))\n",
    "__output_log(log_path, get_settings_str())\n",
    "\n",
    "print('Log output: {}'.format(log_path))\n",
    "loop = tqdm_notebook(range(1000000000))\n",
    "loop.set_description('start')\n",
    "\n",
    "lr = 0.5\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.9, patience=3, verbose=True)\n",
    "\n",
    "losses = {}\n",
    "grads = []\n",
    "\n",
    "images_final = None\n",
    "images_alpha_final = None\n",
    "for e in loop:\n",
    "    t0 = time.time()\n",
    "    \n",
    "    images_alpha = np.empty((model.n_batch*model.batch_size, model.image_size, model.image_size))\n",
    "    images = np.empty((model.n_batch*model.batch_size, model.image_size, model.image_size))\n",
    "    \n",
    "    save_plot = (e % 5 == 0)\n",
    "    losses_minibatch = {}\n",
    "    grad_norm_mean = 0.0\n",
    "    for batch_idx in range(model.n_batch):\n",
    "        i0 = batch_idx*model.batch_size\n",
    "        i1 = i0 + model.batch_size\n",
    "        \n",
    "        model.deform_meshes()\n",
    "        \n",
    "        # ================== #\n",
    "        # train\n",
    "        # ================== #\n",
    "        optimizer.zero_grad()\n",
    "        imgs, l, l_dict = model(batch_idx, learn_texturemap=False, learn_deform=True)\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        images[i0:i1] = imgs['currents_with_bg'].squeeze().detach().cpu().numpy()\n",
    "        images_alpha[i0:i1] = imgs['currents'][..., 3].squeeze().detach().cpu().numpy()\n",
    "        \n",
    "        for k, v in l_dict.items():\n",
    "            if k not in losses_minibatch:\n",
    "                losses_minibatch[k] = 0.0\n",
    "            else:\n",
    "                losses_minibatch[k] += (v.detach().cpu().numpy() / model.n_batch)\n",
    "        \n",
    "        grad_value = model.dnormals.grad.detach().cpu().numpy()\n",
    "        grad_norm_mean += (np.linalg.norm(grad_value) / model.n_batch)\n",
    "        lr_curr = optimizer.param_groups[0]['lr']\n",
    "        loop.set_description('[{}] batch[{}/{}] lr={:.4f}, loss={:.6f}\\n'.format(e, batch_idx+1, model.n_batch, lr_curr, losses_minibatch['total']))\n",
    "        scheduler.step(losses_minibatch['total'])\n",
    "    grads.append(grad_norm_mean)\n",
    "    for k, v in losses_minibatch.items():\n",
    "        if k not in losses:\n",
    "            losses[k] = [v]\n",
    "        else:\n",
    "            losses[k].append(v)\n",
    "\n",
    "    t1 = time.time()\n",
    "    # log\n",
    "    mb_alloc = torch.cuda.memory_allocated() * 0.000001\n",
    "    mb_cached = torch.cuda.memory_cached() * 0.000001\n",
    "    now = datetime.now()\n",
    "    hour = str(now.hour)\n",
    "    minute = str(now.minute)\n",
    "    second = str(now.second)\n",
    "    now_str = '{:>02}:{:>02}:{:>02}'.format(hour, minute, second)\n",
    "    out_str = '{} | {:04} | {:.2f}s | lr={:.8f} | loss={:.6f} | GPU_allocated({:,.2f}Mb) | GPU_cached({:,.2f}Mb)\\n'.format(now_str, e, t1-t0, lr_curr, losses_minibatch['total'], mb_alloc, mb_cached)\n",
    "    __output_log(log_path, out_str)\n",
    "\n",
    "    images_final = images.copy()\n",
    "    images_alpha_final = images_alpha.copy()\n",
    "    \n",
    "    if save_plot:\n",
    "        out_idx = 8\n",
    "        texturemap = model.texture_map.clone().squeeze().detach().cpu().numpy()\n",
    "        img_name = model.batch_dict['img_name'][out_idx]\n",
    "        cam_idx = model.batch_dict['cam_idx'][out_idx]\n",
    "        img_curr = cv2.flip(images_final[out_idx], -1)\n",
    "        img_sil = cv2.flip(images_alpha_final[out_idx], -1)\n",
    "        img_sil = np.dstack([np.where(img_sil > 0, 1, 0), np.zeros(img_sil.shape), np.zeros(img_sil.shape)])\n",
    "        img_target = cv2.flip(targets[out_idx], -1)\n",
    "        plot_img = plot_dverts(out_dir, e, batch_idx, lr_curr, img_name, cam_idx, losses, grads, img_curr, img_target, img_sil)\n",
    "        \n",
    "        plt.figure(figsize=(30, 6))\n",
    "        plt.imshow(plot_img)\n",
    "        plt.xticks([]), plt.yticks([])\n",
    "        plt.show()\n",
    "\n",
    "        # save texturemap\n",
    "#         texels_np = model.texture_map.clone().detach().cpu().numpy()\n",
    "#         np.save(out_dir + '/texturemap.npy', texels_np)\n",
    "#         del texels_np\n",
    "\n",
    "#         texturemap_out = (255.0*np.clip(model.texture_map.detach().squeeze().cpu().numpy(), a_min=0, a_max=1.0)).astype(np.uint8)\n",
    "#         im = Image.fromarray(texturemap_out)\n",
    "#         im.save(out_dir + '/texturemap_learned.png', dpi=(600, 600))\n",
    "    if lr_curr < 1e-10 or e >= 30:\n",
    "        print('lr={}. BREAK'.format(lr_curr))\n",
    "        model.export_obj(out_dir, vt_path=vt_path, export_texturemap=(e == 0), fname_suffix='')\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### export objs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.export_obj(out_dir, vt_path=vt_path, export_texturemap=(e == 0), fname_suffix='')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save overlays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_original = []\n",
    "for k, vs in img_refs.items():\n",
    "    for v in vs:\n",
    "        targets_original.append(v)\n",
    "targets_original = np.float32(targets_original)\n",
    "print(targets_original.shape)\n",
    "print(targets.shape)\n",
    "plt.figure()\n",
    "plt.imshow(targets_original[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tstart = time.time()\n",
    "for i in range(len(images_final)):\n",
    "    t0 = time.time()\n",
    "    image_curr1 = cv2.flip(images_initial[i].squeeze(), -1)\n",
    "    img_sil1 = cv2.flip(images_alpha_initial[i], -1)\n",
    "    img_sil1 = np.dstack([np.where(img_sil1 > 0, 1, 0), np.zeros(img_sil1.shape), np.zeros(img_sil1.shape)])\n",
    "\n",
    "    image_curr2 = cv2.flip(images_final[i].squeeze(), -1)\n",
    "    img_sil2 = cv2.flip(images_alpha_final[i], -1)\n",
    "    img_sil2 = np.dstack([np.where(img_sil2 > 0, 1, 0), np.zeros(img_sil2.shape), np.zeros(img_sil2.shape)])\n",
    "\n",
    "    image_target = cv2.flip(targets_original[i].squeeze(), -1)\n",
    "\n",
    "    cam_idx = model.batch_dict['cam_idx'][i]\n",
    "    img_name = model.batch_dict['img_name'][i]\n",
    "    _ = plot_3_dverts(out_dir, '1initial', i, len(images), img_name, cam_idx, image_target, image_curr1, img_sil1)\n",
    "    plot_img = plot_3_dverts(out_dir, '2final', i, len(images), img_name, cam_idx, image_target, image_curr2, img_sil2)\n",
    "    t1 = time.time()\n",
    "    print('  {}/{}({:.2f}s)'.format(i+1, len(images_final), t1-t0), end='')\n",
    "print('\\nDone: {:.2f}s'.format(time.time()-tstart))\n",
    "plt.figure(figsize=(18, 6))\n",
    "plt.imshow(plot_img)\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### merge two sets of images for comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_names = ['03052', '03067', '04917', '06250', '06550']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shutil import copyfile\n",
    "\n",
    "for img_name in img_names:\n",
    "    dir1 = r'D:\\1_Projects\\200325_PyTorch3d_Projects\\16g_data\\output\\200604_LossCompare\\pixel_lappyr_{}'.format(img_name)\n",
    "    dir2 = r'D:\\1_Projects\\200325_PyTorch3d_Projects\\16g_data\\output\\200604_LossCompare\\lappyr_{}'.format(img_name)\n",
    "\n",
    "    new_dir = r'D:\\1_Projects\\200325_PyTorch3d_Projects\\16g_data\\output\\200604_LossCompare\\compare\\pixel_lappyr_vs_lappyr'\n",
    "\n",
    "    images1 = glob.glob(dir1 + '/*final.png')\n",
    "    for i, src in enumerate(images1):\n",
    "        dst = new_dir + '/{}_{}a.png'.format(img_name, i)\n",
    "        copyfile(src, dst)\n",
    "        print(src ,'->', dst)\n",
    "    print()\n",
    "    images2 = glob.glob(dir2 + '/*final.png')\n",
    "    for i, src in enumerate(images2):\n",
    "        dst = new_dir + '/{}_{}b.png'.format(img_name, i)\n",
    "        copyfile(src, dst)\n",
    "        print(src ,'->', dst)\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "x = np.arange(0, 1, 0.01)\n",
    "y = np.arange(0, 1, 0.01)\n",
    "\n",
    "X, Y = np.meshgrid(x, y)\n",
    "L = 1 - abs(X*Y)/(abs(X+Y-X*Y)+1e-8)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.set_xlabel('I_1')\n",
    "ax.set_ylabel('I_2')\n",
    "ax.set_zlabel('L')\n",
    "ax.plot_surface(X, Y, L)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
