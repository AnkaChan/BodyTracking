{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:96% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04/18 21:30:23]\n",
      "1.4.0\n",
      "9.2\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:96% !important; }</style>\"))\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.io import imread\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from skimage import img_as_ubyte\n",
    "import imageio\n",
    "import json\n",
    "import cv2\n",
    "import time\n",
    "from PIL import Image\n",
    "from pytorch3d.loss import (\n",
    "    mesh_laplacian_smoothing, \n",
    "    mesh_normal_consistency,\n",
    ")\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "# Util function for loading meshes\n",
    "from pytorch3d.io import load_objs_as_meshes, load_obj\n",
    "import math\n",
    "# Data structures and functions for rendering\n",
    "from pytorch3d.structures import Meshes, Textures, join_meshes\n",
    "from pytorch3d.renderer import (\n",
    "    look_at_view_transform,\n",
    "    OpenGLPerspectiveCameras, \n",
    "    SfMPerspectiveCameras,\n",
    "    PointLights, \n",
    "    DirectionalLights,\n",
    "    Materials, \n",
    "    RasterizationSettings, \n",
    "    MeshRenderer, \n",
    "    MeshRasterizer,  \n",
    "    TexturedSoftPhongShader,\n",
    "    SoftSilhouetteShader,\n",
    "    look_at_rotation,\n",
    "    HardFlatShader\n",
    ")\n",
    "\n",
    "# add path for demo utils functions \n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "sys.path.append(os.path.abspath(''))\n",
    "\n",
    "from datetime import datetime\n",
    "def now_str():\n",
    "    now = datetime.now()\n",
    "    month = str(now.month)\n",
    "    day = str(now.day)\n",
    "    hour = str(now.hour)\n",
    "    minute = str(now.minute)\n",
    "    sec = str(now.second)\n",
    "    \n",
    "    output = '[{:>02}/{:>02} {:>02}:{:>02}:{:>02}]'.format(month, day, hour, minute, sec)\n",
    "    return output\n",
    "def __output_log(path, strs):\n",
    "    if not os.path.exists(path):\n",
    "        with open(path, 'w+') as f:\n",
    "            f.write(strs)\n",
    "            f.close()\n",
    "    else:\n",
    "        with open(path, 'a+') as f:\n",
    "            f.write(strs)\n",
    "            f.close()\n",
    "print(now_str())\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "def reproject(params, vertices, distort=False):\n",
    "    R = params['R']\n",
    "    T = params['T']\n",
    "    fx = params['fx']\n",
    "    fy = params['fy']\n",
    "    cx = params['cx']\n",
    "    cy = params['cy']\n",
    "\n",
    "    E = np.array([\n",
    "        [R[0,0], R[0,1], R[0,2], T[0]], \n",
    "        [R[1,0], R[1,1], R[1,2], T[1]], \n",
    "        [R[2,0], R[2,1], R[2,2], T[2]], \n",
    "        [0, 0, 0, 1]]).astype('double')\n",
    "    \n",
    "    if distort:\n",
    "        k1 = params['k1']\n",
    "        k2 = params['k2']\n",
    "        k3 = params['k3']\n",
    "        p1 = params['p1']\n",
    "        p2 = params['p2']\n",
    "        \n",
    "    img_pts = []\n",
    "    for i in range(len(vertices)):\n",
    "        v = np.array(vertices[i])\n",
    "\n",
    "        # extrinsics\n",
    "        v4 = E.dot(np.array([v[0], v[1], v[2], 1]).astype('double'))\n",
    "        xp = v4[0] / v4[2]\n",
    "        yp = v4[1] / v4[2]\n",
    "\n",
    "        if distort:\n",
    "            # intrinsics\n",
    "            r2 = xp**2 + yp**2\n",
    "            ## radial\n",
    "            radial_dist = 1 + k1*(r2) + k2*(r2*r2) + k3*(r2*r2*r2)\n",
    "\n",
    "            ## tangential\n",
    "            tan_x = p2 * (r2 + 2.0 * xp * xp) + 2.0 * p1 * xp * yp\n",
    "            tan_y = p1 * (r2 + 2.0 * yp * yp) + 2.0 * p2 * xp * yp\n",
    "\n",
    "            xp = xp * radial_dist + tan_x\n",
    "            yp = yp * radial_dist + tan_y\n",
    "            \n",
    "        u = fx * xp + cx\n",
    "        v = fy * yp + cy\n",
    "        pr = 1\n",
    "        nr = 0\n",
    "        if (-4000*nr < u and u < pr*4000) and (-2160*nr < v and v < pr*2160):\n",
    "            img_pts.append(np.array([u, v]))\n",
    "    img_pts = np.array(img_pts)\n",
    "    return img_pts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.is_available(): True\n",
      "torch.cuda.current_device(): 0\n",
      "torch.cuda.get_device_name(0): GeForce RTX 2070 SUPER\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n",
      "torch.cuda.memory_reserved(): 0.00 Mb\n",
      "torch.cuda.memory_allocated(): 0.00 Mb\n",
      "torch.cuda.memory_cached(): 0.00 Mb\n",
      "----- torch.cuda.empty_cache() -----\n",
      "torch.cuda.memory_reserved(): 0.00 Mb\n",
      "torch.cuda.memory_allocated(): 0.00 Mb\n",
      "torch.cuda.memory_cached(): 0.00 Mb\n"
     ]
    }
   ],
   "source": [
    "print('torch.cuda.is_available():',torch.cuda.is_available())\n",
    "\n",
    "device_gpu = torch.device(\"cuda:0\")\n",
    "torch.cuda.set_device(device_gpu)\n",
    "device_cpu = torch.device('cpu')\n",
    "\n",
    "print('torch.cuda.current_device():', torch.cuda.current_device())\n",
    "torch.cuda.ipc_collect()\n",
    "print('torch.cuda.get_device_name(0):',torch.cuda.get_device_name(0))\n",
    "\n",
    "# print('GPU memory stats ---------------------')\n",
    "# gpu_mem_stats = torch.cuda.memory_stats(device=device_gpu)\n",
    "# for k, v in gpu_mem_stats.items():\n",
    "#     print('  {}: {}'.format(k, v))\n",
    "\n",
    "print(torch.cuda.memory_summary(device=device_gpu, abbreviated=False))\n",
    "bytes_reserved = torch.cuda.memory_reserved()\n",
    "print('torch.cuda.memory_reserved(): {:,.2f} Mb'.format(bytes_reserved * 0.000001))\n",
    "# Returns the current GPU memory usage by \n",
    "# tensors in bytes for a given device\n",
    "bytes_alloc = torch.cuda.memory_allocated()\n",
    "print('torch.cuda.memory_allocated(): {:,.2f} Mb'.format(bytes_alloc * 0.000001))\n",
    "\n",
    "# Returns the current GPU memory managed by the\n",
    "# caching allocator in bytes for a given device\n",
    "bytes_cached = torch.cuda.memory_cached()\n",
    "print('torch.cuda.memory_cached(): {:,.2f} Mb'.format(bytes_cached * 0.000001))\n",
    "\n",
    "\n",
    "# Releases all unoccupied cached memory currently held by\n",
    "# the caching allocator so that those can be used in other\n",
    "# GPU application and visible in nvidia-smi\n",
    "print('----- torch.cuda.empty_cache() -----')\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "bytes_reserved = torch.cuda.memory_reserved()\n",
    "print('torch.cuda.memory_reserved(): {:,.2f} Mb'.format(bytes_reserved * 0.000001))\n",
    "bytes_alloc = torch.cuda.memory_allocated()\n",
    "print('torch.cuda.memory_allocated(): {:,.2f} Mb'.format(bytes_alloc * 0.000001))\n",
    "bytes_cached = torch.cuda.memory_cached()\n",
    "print('torch.cuda.memory_cached(): {:,.2f} Mb'.format(bytes_cached * 0.000001))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6890, 3])\n",
      "  - batch size: 1, num_batches: 16\n",
      "  - cameras   : 16\n",
      "  - renderers   : 16\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "images_rendered: (16, 32, 32, 4)\n",
      "tensor(1004.3762, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, device, **kwargs):\n",
    "        \"\"\"\n",
    "        image_size: a scalar. Only square image is supported in PyTorch3d\n",
    "        \"\"\"\n",
    "        stat_str = ''\n",
    "        \n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.cams = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P']\n",
    "\n",
    "        self.image_size = kwargs.get('image_size', None)\n",
    "        \n",
    "        # set ref images: [0, 255] uint8\n",
    "        image_refs = kwargs.get('image_refs', None)\n",
    "        self.image_refs = torch.from_numpy(np.array(image_refs).astype(np.uint8)).to(self.device)\n",
    "        \n",
    "        # load texturemaps: [0.0, 1.0] float\n",
    "        texturemap_shape = kwargs.get('texturemap_shape', None)\n",
    "        texture_map_torch = torch.from_numpy(np.random.rand(1, texturemap_shape[0], texturemap_shape[1], texturemap_shape[2]).astype(np.float32)).to(self.device)\n",
    "        self.texture_map = nn.Parameter(texture_map_torch, requires_grad=True)\n",
    "        \n",
    "        # batch_size\n",
    "        self.batch_size = kwargs.get('batch_size', None)\n",
    "        self.num_renders = len(self.image_refs)\n",
    "        self.n_batch = int(self.num_renders / self.batch_size)\n",
    "        \n",
    "        # set mesh\n",
    "        mesh_path = kwargs.get('mesh_path', None)\n",
    "        self.meshes = self._load_meshes_list(self.device, [mesh_path], texture_map_torch)\n",
    "        \n",
    "        # vertex deformations\n",
    "        self.vert_normals = self.meshes[0].verts_normals_packed()\n",
    "        deform_verts = torch.from_numpy(np.zeros((self.vert_normals.shape), dtype=np.float32)).to(self.device)\n",
    "        self.deform_verts = nn.Parameter(deform_verts, requires_grad=True)\n",
    "        \n",
    "        # init renderers\n",
    "        cam_pararms = kwargs.get('cam_params', None)\n",
    "        self.batch_renderers = self._init_renderer(cam_params=cam_pararms, batch_size=self.batch_size, n_batch=self.n_batch)\n",
    "        \n",
    "    def _load_meshes_list(self, device, mesh_paths, texture_map):\n",
    "        meshes_list = []\n",
    "        for path in mesh_paths:\n",
    "            meshes_list.append(load_objs_as_meshes([path], universal_texturemap=texture_map.cpu(), device=device))\n",
    "        return meshes_list\n",
    "        \n",
    "    def forward(self):\n",
    "        loss = 0\n",
    "        losses_dict = {'pixel': 0.0, 'mesh_normal': 0.0, 'mesh_laplacian': 0.0}\n",
    "        stat_str = ''\n",
    "        log = ''\n",
    "        t0 = time.time()\n",
    "        \n",
    "        deformed_mesh = self.meshes[0].offset_verts(self.deform_verts)\n",
    "        meshes = deformed_mesh.extend(self.batch_size)\n",
    "        \n",
    "        images_out = torch.zeros(self.image_refs.shape[0], self.image_refs.shape[1], self.image_refs.shape[2], 4)\n",
    "        t1 = time.time()\n",
    "        log += 'forward\\n'\n",
    "        log += ' - {:<10}: {:.3f}s\\n'.format('data prep', t1 - t0)\n",
    "        for batch_idx in range(self.n_batch):\n",
    "            t2 = time.time()\n",
    "            i0 = batch_idx*batch_size\n",
    "            i1 = i0 + batch_size\n",
    "            \n",
    "            # [0.0, 1.0] float32 in gpu -> change to [0.0, 255.0] cpu. shape (batch_size, W, H, 4)\n",
    "            image_cur, _ = self.batch_renderers[batch_idx](meshes_world=meshes, texture_maps=self.texture_map)\n",
    "            # save for output\n",
    "            images_out[i0:i1] = image_cur\n",
    "            \n",
    "            # [0, 255] float32 in cpu\n",
    "            image_refs = self.image_refs[i0:i1]\n",
    "\n",
    "            print(image_cur.shape)\n",
    "            print(image_refs.shape)\n",
    "            loss_i = (torch.mean((image_cur[...,0] - image_refs) ** 2) / 255.0)\n",
    "            loss = loss + loss_i\n",
    "            \n",
    "            losses_dict['pixel'] += loss_i\n",
    "            t3 = time.time()\n",
    "            log += ' - [{}/{}] {:<10}: {:.3f}s\\n'.format(batch_idx+1, self.n_batch, 'batch render', t3 - t2)\n",
    "            log += '   ------------------------------------\\n'\n",
    "            log += '   ------------------------------------\\n'\n",
    "        loss_normal = 0.0\n",
    "        loss_laplacian = 0.0\n",
    "        \n",
    "        losses_dict['mesh_normal'] += loss_normal\n",
    "        losses_dict['mesh_laplacian'] += loss_laplacian\n",
    "            \n",
    "        loss = 1.0*loss + 0.5*loss_normal + 0.5*loss_laplacian\n",
    "        return loss, images_out, self.texture_map, stat_str, losses_dict, log\n",
    "\n",
    "    def get_gpu_stats(self, output_str=True):\n",
    "        mb_reserved = torch.cuda.memory_reserved() * 0.000001\n",
    "        mb_alloc = torch.cuda.memory_allocated() * 0.000001\n",
    "        mb_cached = torch.cuda.memory_cached() * 0.000001\n",
    "        if output_str:\n",
    "            return 'alloc={:,.0f}MB | cached={:,.0f}MB | reserved={:,.0f}MB'.format(mb_alloc, mb_cached, mb_reserved)\n",
    "        else:\n",
    "            return mb_alloc, mb_cached, mb_reserved\n",
    "        \n",
    "    def _load_mesh(self, device, mesh_paths, texturemap_torch):\n",
    "        # load mesh\n",
    "        meshes = load_objs_as_meshes(mesh_paths, universal_texturemap=texturemap_torch, device=device)\n",
    "        return meshes\n",
    "    \n",
    "    def save_parameters(self, out_path):\n",
    "        deform_verts = self.deform_verts.detach().cpu().numpy()\n",
    "        np.save(out_path, deform_verts)\n",
    "        print('Parameters saved:', out_path)\n",
    "        \n",
    "    def load_parameters(self, in_path):\n",
    "        self.deform_verts = nn.Parameter(torch.from_numpy(np.load(in_path)).to(self.device))\n",
    "        print('Parameters loaded: {}'.format(self.deform_verts.shape))\n",
    "        \n",
    "    def export_obj(self, out_path):\n",
    "        normal_deforms = self.deform_verts * self.vert_normals\n",
    "        deformed_mesh = self.mesh.offset_verts(normal_deforms)\n",
    "\n",
    "        verts = deformed_mesh.verts_packed()\n",
    "        faces = deformed_mesh.faces_packed()\n",
    "        vnormals = deformed_mesh.verts_normals_list()[0]\n",
    "        fnormals = deformed_mesh.faces_normals_list()[0]\n",
    "\n",
    "        assert(faces.shape[0] == fnormals.shape[0])\n",
    "        assert(vnormals.shape[0] == verts.shape[0])\n",
    "\n",
    "        with open(out_path, 'w+') as f:\n",
    "            f.write('# OBJ file created by Hyojoon Park.\\n')\n",
    "            f.write('###########################\\n')\n",
    "            f.write('# Vertices:       {}\\n'.format(verts.shape[0]))\n",
    "            f.write('# Vertex normals: {}\\n'.format(vnormals.shape[0]))\n",
    "            f.write('# Faces:          {}\\n'.format(faces.shape[0]))\n",
    "            f.write('###########################\\n')\n",
    "\n",
    "            for i in range(verts.shape[0]):\n",
    "                f.write('vn {:.4f} {:.4f} {:.4f}\\n'.format(vnormals[i][0], vnormals[i][1], vnormals[i][2]))\n",
    "                f.write('v {:.4f} {:.4f} {:.4f}\\n'.format(verts[i][0], verts[i][1], verts[i][2]))\n",
    "\n",
    "            for i in range(faces.shape[0]):\n",
    "                f.write(\"f\")\n",
    "                face = faces[i, :]\n",
    "                for fi in range(face.shape[0]):\n",
    "                    f.write(' {0:.0f}//{0:.0f}'.format(face[fi] + 1, fnormals[fi] + 1))\n",
    "#                     f.write(' {0:.0f}'.format(face[fi]))\n",
    "                f.write(\"\\n\")\n",
    "\n",
    "        print('Obj exported to:', out_path)\n",
    "        \n",
    "    def _init_renderer(self, cam_params, batch_size, n_batch):\n",
    "        batch_renderers = []\n",
    "        \n",
    "        raster_settings = RasterizationSettings(\n",
    "            image_size=self.image_size, \n",
    "            blur_radius=0.0, \n",
    "            faces_per_pixel=1, \n",
    "            bin_size = 0, # this setting controls whether naive or coarse-to-fine rasterization is used\n",
    "            max_faces_per_bin = None  # this setting is for coarse rasterization\n",
    "        )\n",
    "        \n",
    "        print('  - batch size: {}, num_batches: {}'.format(batch_size, n_batch))\n",
    "        print('  - cameras   : {}'.format(cam_params['T'].shape[0]))\n",
    "        locations = torch.from_numpy(np.array([0, 0, 3000])).to(self.device)\n",
    "        a_diffuse = 0\n",
    "        a_ambient = 0.5\n",
    "        s = torch.from_numpy(np.zeros((1, 3)).astype(np.float32)).to(self.device)\n",
    "        d = torch.from_numpy(np.ones((1, 3)).astype(np.float32)*a_diffuse).to(self.device)\n",
    "        a = torch.from_numpy(np.ones((1, 3)).astype(np.float32)*a_ambient).to(self.device)\n",
    "        light = PointLights(device=self.device, location=[[1000, 1000, 1000]], specular_color=s, ambient_color=a, diffuse_color=d)\n",
    "        light.location = locations\n",
    "        light.specular_color = s\n",
    "        light.diffuse_color = d\n",
    "        light.ambient_color = a\n",
    "        \n",
    "        for batch_idx in range(n_batch):\n",
    "            i0 = batch_idx*batch_size\n",
    "            i1 = i0 + batch_size\n",
    "            R = cam_params['R'][i0:i1]\n",
    "            T = cam_params['T'][i0:i1]\n",
    "            focal_length = cam_params['fl'][i0:i1]\n",
    "            principal_point = cam_params['pp'][i0:i1]\n",
    "            cameras = SfMPerspectiveCameras(device=self.device, R=R, T=T, principal_point=principal_point, focal_length=focal_length)\n",
    "\n",
    "            renderer = MeshRenderer(\n",
    "                    rasterizer=MeshRasterizer(\n",
    "                        cameras=cameras,\n",
    "                        raster_settings=raster_settings\n",
    "                    ),\n",
    "                    shader=TexturedSoftPhongShader(\n",
    "                        device=self.device, \n",
    "                        cameras=cameras,\n",
    "                        lights=light\n",
    "                    )\n",
    "                )\n",
    "            batch_renderers.append(renderer)\n",
    "        print('  - renderers   : {}'.format(len(batch_renderers)))\n",
    "        return batch_renderers\n",
    "    \n",
    "    def _merge_fg_bg(self, fg, bg):\n",
    "        \"\"\"\n",
    "        fg: mesh rendering. [N, W, H, 4]: [0, 1.0] float\n",
    "        bg: clean plate. [N, W, H]: [0, 255] uint8\n",
    "        \"\"\"\n",
    "        out = torch.where(fg[...,0] < 1.0, 255.0*fg[...,0], bg)\n",
    "        if len(out.shape) < 3:\n",
    "            out = out.unsqueeze(0)\n",
    "        return out\n",
    "    \n",
    "model = Model(device_gpu, img_dir=img_dir, cam_params=cams_torch, image_refs=img_refs, texturemap_shape=texturemap_shape, mesh_path=mesh_path, image_size=image_size, batch_size=batch_size)\n",
    "\n",
    "lr = 1.0\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "optimizer.zero_grad()\n",
    "loss, images_rendered, texture_map, stats_gpu, loss_dict, log = model()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "images_rendered = images_rendered.detach().squeeze().cpu().numpy()\n",
    "print('images_rendered:',images_rendered.shape)\n",
    "plt.imshow(images_rendered[0])\n",
    "print(loss)\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load mesh, camera, images, and feed into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cameras(cam_path, device, actual_img_shape):\n",
    "    h = actual_img_shape[0]\n",
    "    w = actual_img_shape[1]\n",
    "    img_size = min(w, h)\n",
    "    \n",
    "    # load cameras\n",
    "    cams = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P']\n",
    "\n",
    "    with open(cam_path, 'r') as f:\n",
    "        j = json.load(f)\n",
    "        camera_params = j['cam_params']\n",
    "\n",
    "    cameras = []\n",
    "    cam_params = []\n",
    "    cam_pos = []\n",
    "    Rs, Ts, focal_lengths, principal_points = [], [], [], []\n",
    "    for cam_idx, cam in enumerate(cams):\n",
    "        cam_param = camera_params[str(cam_idx)]\n",
    "        # for undistortion\n",
    "        fx = cam_param['fx']\n",
    "        fy = cam_param['fy']\n",
    "        cx = cam_param['cx']\n",
    "        cy = cam_param['cy']\n",
    "        k1 = cam_param['k1']\n",
    "        k2 = cam_param['k2']\n",
    "        k3 = cam_param['k3']\n",
    "        p1 = cam_param['p1']\n",
    "        p2 = cam_param['p2']\n",
    "        \n",
    "        rvec = np.float32(cam_param['rvec'])\n",
    "        T = np.float32(cam_param['tvec'])\n",
    "        R, _ = cv2.Rodrigues(rvec)\n",
    "        Rs.append(R.T)\n",
    "        Ts.append(T)\n",
    "        \n",
    "        if cam_idx % 6 == 0:\n",
    "            cam_pos.append(-R.T.dot(T))\n",
    "        \n",
    "        cx_corrected = cx*2/img_size - w/img_size\n",
    "        cy_corrected = cy*2/img_size - h/img_size\n",
    "        fx_corrected = fx*2/img_size\n",
    "        fy_corrected = fy*2/img_size\n",
    "        principal_point = np.array([cx_corrected, cy_corrected]).astype(np.float32)\n",
    "        focal_length = np.array([fx_corrected, fy_corrected]).astype(np.float32)\n",
    "        focal_lengths.append(focal_length)\n",
    "        principal_points.append(principal_point)\n",
    "\n",
    "        K = np.float32([[fx, 0, cx], [0, fy, cy], [0, 0, 1]])\n",
    "        dist = np.float32([k1, k2, p1, p2, k3])\n",
    "        cam_params.append({'K': K, 'dist': dist, 'R': R, 'T': T, 'fx': fx, 'fy': fy, 'cx': cx, 'cy': cy})\n",
    "    \n",
    "    R_torch = torch.from_numpy(np.array(Rs).astype(np.float32))\n",
    "    T_torch = torch.from_numpy(np.array(Ts).astype(np.float32))\n",
    "    focal_length = torch.from_numpy(np.array(focal_lengths).astype(np.float32))\n",
    "    principal_point = torch.from_numpy(np.array(principal_points).astype(np.float32))\n",
    "    out_for_torch = {'R': R_torch, 'T': T_torch, 'fl': focal_length, 'pp': principal_point}\n",
    "    return cameras, cam_params, out_for_torch\n",
    "\n",
    "def load_images(img_dir, cam_params):\n",
    "    img_paths = sorted(glob.glob(img_dir + '/*.jpg'))\n",
    "    image_refs0 = []\n",
    "    image_refs_undistort = []\n",
    "    for i, path in enumerate(img_paths):\n",
    "        # img = imageio.imread(path)\n",
    "        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "        img_undist = _undistort(img, cam_params[i])\n",
    "        image_refs0.append(img)\n",
    "        image_refs_undistort.append(img_undist)\n",
    "\n",
    "    w = 2160 / 2\n",
    "    image_refs_cropped = []\n",
    "    for i in range(len(image_refs_undistort)):\n",
    "        image = image_refs_undistort[i]\n",
    "        cx = image.shape[1] / 2\n",
    "\n",
    "        image = image_refs_undistort[i].astype(np.uint8)\n",
    "        img = image[:, int(cx-w):int(cx+w)]\n",
    "        img = cv2.resize(img, (image_size, image_size))\n",
    "        img = cv2.flip(img, -1)\n",
    "        image_refs_cropped.append(img)\n",
    "    return image_refs0, image_refs_undistort, image_refs_cropped\n",
    "                         \n",
    "def _undistort(img, cam_param):\n",
    "    # undistort a single image)\n",
    "    h, w = img.shape[:2]\n",
    "    # cv2.undistort(src, cameraMatrix, distCoeffs[, dst[, newCameraMatrix]]) â†’ dst\n",
    "    img = cv2.undistort(img, cam_param['K'], cam_param['dist'], None, None)\n",
    "    return img\n",
    "                         \n",
    "def load_clean_plates(img_dir, cam_params):\n",
    "    img_paths = sorted(glob.glob(img_dir + '/*.pgm'))\n",
    "    images0 = []\n",
    "    images_undistort = []\n",
    "    for i, path in enumerate(img_paths):\n",
    "        # img = imageio.imread(path)\n",
    "        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "        img_undist = _undistort(img, cam_params[i])\n",
    "        images0.append(img)\n",
    "        images_undistort.append(img_undist)\n",
    "\n",
    "    w = 2160 / 2\n",
    "    clean_plates_cropped = []\n",
    "    for i in range(len(images_undistort)):\n",
    "        image = images_undistort[i]\n",
    "        cx = image.shape[1] / 2\n",
    "\n",
    "        image = images_undistort[i].astype(np.uint8)\n",
    "        img = image[:, int(cx-w):int(cx+w)]\n",
    "        img = cv2.resize(img, (image_size, image_size))\n",
    "        img = cv2.flip(img, -1)\n",
    "        # img = np.dstack([img, img, img])\n",
    "        clean_plates_cropped.append(img)\n",
    "    return images0, images_undistort, clean_plates_cropped "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cams = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P']\n",
    "cam_path = './10b_data/input/cam_params.json'\n",
    "mesh_path = './4_data/input/2020_03_28_SMPL_UV/SMPL_registration/SMPLFit_TPose.obj'\n",
    "img_dir = './4_data/input/2020_03_28_SMPL_UV/SMPL_registration/TPose'\n",
    "# clean_plate_dir = r'D:\\200330_ToJanKeller\\data\\input\\clean_plate'\n",
    "\n",
    "# number of batch will be automatically computed\n",
    "batch_size = 1\n",
    "\n",
    "texturemap_shape = (32, 32, 1)\n",
    "image_size = 32\n",
    "\n",
    "# input image size\n",
    "actual_img_shape = (2160, 4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cam_path = './4_data/input/CameraCalibration/2020_03_22_NewSuitDesignCapture\\FinalCamParams\\cam_params.json'\n",
    "# mesh_path = './4_data/input/2020_03_28_SMPL_UV/SMPL_registration/SMPLFit_TPose.obj'\n",
    "# img_dir = './4_data/input/2020_03_28_SMPL_UV/SMPL_registration/TPose'\n",
    "# texturemap_shape = (2160, 2160, 1)\n",
    "# image_size = 2160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2160, 4000)\n"
     ]
    }
   ],
   "source": [
    "device = device_gpu\n",
    "cameras, cam_params, cams_torch = load_cameras(cam_path, device, actual_img_shape=actual_img_shape)\n",
    "img_refs_original, img_refs_undistorted, img_refs = load_images(img_dir, cam_params)\n",
    "print(img_refs_undistorted[0].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = img_refs_original[0]\n",
    "print('Image original =========')\n",
    "print(img.shape, ',', np.max(img), ',', img.dtype)\n",
    "print('{:,.2f} Mb'.format(img.nbytes * 0.000001))\n",
    "\n",
    "print()\n",
    "img = img_refs[0]\n",
    "print('Image cropped =========')\n",
    "print(img.shape, ',', np.max(img), ',', img.dtype)\n",
    "print('{:,.2f} Mb'.format(img.size * img.itemsize * 0.000001))\n",
    "print()\n",
    "i = 5\n",
    "fig, ax = plt.subplots(1, 3, figsize=(21, 7))\n",
    "ax[0].imshow(img_refs_original[i], cmap='gray')\n",
    "ax[0].set_title('original')\n",
    "ax[1].imshow(img_refs_undistorted[i], cmap='gray')\n",
    "ax[1].set_title('undistorted')\n",
    "ax[2].imshow(img_refs[i], cmap='gray')\n",
    "ax[2].set_title('undistorted & cropped')\n",
    "ax[2].invert_yaxis()\n",
    "plt.show()\n",
    "\n",
    "del img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = Model(device_gpu, img_dir=img_dir, cam_params=cams_torch, image_refs=img_refs, texturemap_shape=texturemap_shape, mesh_path=mesh_path, image_size=image_size, batch_size=batch_size)\n",
    "texture_maps = model.texture_map\n",
    "print('texture_map: {:,.2f}=={:,.2f} Mb'.format(texture_maps.element_size() * texture_maps.nelement() * 0.000001, texture_maps.detach().cpu().numpy().nbytes*0.000001))\n",
    "texture_maps_np = texture_maps.detach().cpu().numpy()\n",
    "print('  {} {:,.2f}Mb {} {}'.format(texture_maps.shape, texture_maps_np.nbytes*0.000001, texture_maps.dtype, np.max(texture_maps_np)))\n",
    "img = model.image_refs[0]\n",
    "print('ref image: {}, {:,.2f} Mb'.format(img.shape, img.element_size() * img.nelement() * 0.000001))\n",
    "\n",
    "\n",
    "\n",
    "lr = 1.0\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "optimizer.zero_grad()\n",
    "loss, images_rendered, texture_map, stats_gpu, loss_dict, log = model()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "images_rendered = images_rendered.detach().squeeze().cpu().numpy()\n",
    "print('images_rendered:',images_rendered.shape)\n",
    "plt.imshow(images_rendered[0])\n",
    "print(loss)\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pytorch3d mesh rendering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utils import image_grid\n",
    "bytes_alloc = torch.cuda.memory_allocated()\n",
    "print('torch.cuda.memory_allocated(): {:,.2f} Mb'.format(bytes_alloc * 0.000001))\n",
    "bytes_cached = torch.cuda.memory_cached()\n",
    "print('torch.cuda.memory_cached(): {:,.2f} Mb'.format(bytes_cached * 0.000001))\n",
    "\n",
    "t0 = time.time()\n",
    "loss, images_rendered, texture_map, stats_gpu, loss_dict, log = model()\n",
    "t1 = time.time()\n",
    "# model.save_parameters('./7_data/output/deform_verts.npy')\n",
    "# model.export_obj('./7_data/output/obj.obj')\n",
    "t2 = time.time()\n",
    "print('forward:{:.2f}s, save:{:.2f}'.format(t1-t0, t2-t1))\n",
    "\n",
    "bytes_alloc = torch.cuda.memory_allocated()\n",
    "print('torch.cuda.memory_allocated(): {:,.2f} Mb'.format(bytes_alloc * 0.000001))\n",
    "bytes_cached = torch.cuda.memory_cached()\n",
    "print('torch.cuda.memory_cached(): {:,.2f} Mb'.format(bytes_cached * 0.000001))\n",
    "\n",
    "print(log)\n",
    "# mesh from pytorch3d\n",
    "i = 0\n",
    "img_mesh = images_rendered.squeeze().detach().cpu().numpy()\n",
    "print(img_mesh.shape)\n",
    "for batch_idx in range(model.n_batch):\n",
    "    fig, ax = plt.subplots(1, model.batch_size, figsize=(model.batch_size*5, 5))\n",
    "    if model.batch_size > 1:\n",
    "        ax = ax.ravel()\n",
    "        for n in range(model.batch_size):\n",
    "            img_mesh2 = img_mesh[i]\n",
    "            img_mesh2 = cv2.flip(img_mesh2, -1)\n",
    "            ax[n].set_title('batch[{}][{}] | i={}'.format(batch_idx, i, batch_idx*batch_size + i + 1))\n",
    "            ax[n].imshow(img_mesh2, cmap='gray')\n",
    "            i += 1\n",
    "    else:\n",
    "        img_mesh2 = img_mesh[i]\n",
    "        img_mesh2 = cv2.flip(img_mesh2, -1)\n",
    "        ax.set_title('batch[{}][{}] | i={}'.format(batch_idx, i, batch_idx*batch_size + i + 1))\n",
    "        ax.imshow(img_mesh2, cmap='gray')\n",
    "        i += 1\n",
    "    break\n",
    "plt.show()\n",
    "# print(stats_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### add clean plate background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def merge_fg_bg(fg, bg):\n",
    "    \"\"\"\n",
    "    fg: mesh rendering. [W, H, 4]: [0, 1.0] float\n",
    "    bg: clean plate. [W, H]: [0, 255] uint8\n",
    "    \"\"\"\n",
    "    max_pixel = np.max(fg)\n",
    "    out = np.where(fg[...,0] < max_pixel, 255*fg[...,0], bg)\n",
    "    return out\n",
    "    \n",
    "img_idx = 0\n",
    "cp = clean_plates[img_idx]\n",
    "img = img_mesh[img_idx]\n",
    "img2 = merge_fg_bg(img, cp)\n",
    "plt.imshow(img2, cmap='gray', vmin=0, vmax=255)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n",
    "plt.imshow(img, cmap='gray', vmin=0, vmax=255)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reprojections of obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_vertices = model.meshes.verts_packed().cpu()\n",
    "mesh_points = []\n",
    "\n",
    "t0 = time.time()\n",
    "for cam_idx in range(16):\n",
    "    params = cam_params[cam_idx]\n",
    "    pts = reproject(params, mesh_vertices, distort=False)\n",
    "    mesh_points.append(pts)\n",
    "t1 = time.time()\n",
    "print('{:.2f}s'.format(t1-t0))\n",
    "print(pts.shape)\n",
    "print(len(mesh_points))\n",
    "del mesh_vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "for i in range(16):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    # bg\n",
    "    img_bg = img_refs[i]\n",
    "    img_bg = cv2.flip(img_bg, -1)\n",
    "    plt.imshow(img_bg, cmap='gray')\n",
    "    \n",
    "    # mesh from pytorch3d\n",
    "    img_mesh = images_rendered[i].squeeze().detach().cpu().numpy()[..., :3]\n",
    "    img_mesh2 = img_mesh\n",
    "    img_mesh2 = cv2.flip(img_mesh2, -1)\n",
    "    plt.imshow(img_mesh2, alpha=0.5, cmap='gray')\n",
    "    \n",
    "    pts = mesh_points[i]\n",
    "    pts_small_x = (pts[:, 0] - (4000-2160)*0.5) * image_size/2160\n",
    "    pts_small_y = pts[:, 1] * image_size/2160\n",
    "    pts_small = np.stack([pts_small_x, pts_small_y]).T\n",
    "    pts_center = np.mean(pts_small, axis=0)\n",
    "    plt.scatter(pts_small[:, 0], pts_small[:, 1], c='b', s=0.01)\n",
    "    plt.title('Camera {}'.format(i))\n",
    "    \n",
    "    # plot centers\n",
    "    plt.scatter(pts_center[0], pts_center[1], c='r')\n",
    "    \n",
    "    if i == 0:\n",
    "        print('reference image:', np.max(img_bg), img_bg.dtype, img_bg.shape)\n",
    "        print('rendered pytorch image :', np.max(img_mesh), img_mesh.dtype, img_mesh.shape, ', {:,.2f} Mb'.format(img_mesh.nbytes * 0.000001), np.max(img_mesh))\n",
    "        print('rendered pytorch image2:', np.max(img_mesh2), img_mesh2.dtype, img_mesh2.shape, ', {:,.2f} Mb'.format(img_mesh2.nbytes * 0.000001), np.max(img_mesh2))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def visualize_LR(e, lr, losses, images, imgR, out_path):\n",
    "    cams = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P']\n",
    "    img_meshes = []\n",
    "    for i in range(len(images)):\n",
    "        img = images[i].detach().squeeze(0).cpu().numpy()\n",
    "        img_mesh = np.clip(cv2.flip(img, -1), a_min=0.0, a_max=1.0)\n",
    "        img_meshes.append(img_mesh)\n",
    "        \n",
    "    fig = plt.figure(figsize=(18, 3), tight_layout=True)\n",
    "    \n",
    "    gs = fig.add_gridspec(2, 12)\n",
    "    for r in range(2):\n",
    "        for c in range(8):\n",
    "            i = r*8 + c\n",
    "            ax = fig.add_subplot(gs[r, c])\n",
    "            ax.set_title('{}'.format(cams[i]))\n",
    "            ax.imshow(img_meshes[i], cmap='gray')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "    ax = fig.add_subplot(gs[:, 8:10])\n",
    "    ax.plot(losses)\n",
    "    ax.set_title('loss={:.8f}'.format(losses[-1]))\n",
    "    \n",
    "    ax = fig.add_subplot(gs[:, 10:12])\n",
    "    imgR2 = imgR.squeeze().detach().cpu().numpy()\n",
    "    imgR2 = np.clip(imgR2, a_min=0.0, a_max=1.0)\n",
    "    ax.imshow(imgR2, cmap='gray', vmin=0, vmax=1.0)\n",
    "    ax.set_title('Learned Texturemap')\n",
    "    \n",
    "    plt.suptitle('Epoch: {:<10}  lr: {:<.4f}'.format(e, lr))\n",
    "    plt.savefig(out_path, dpi=150)\n",
    "    plt.close('all')\n",
    "    \n",
    "    saved_img = imageio.imread(save_path)\n",
    "    return saved_img\n",
    "\n",
    "def visualize2(e, lr, losses, losses_dict, image1, image2, image3, out_path):\n",
    "    fig, ax = plt.subplots(1, 4, figsize=(24, 6), tight_layout=True)\n",
    "    ax = ax.ravel()\n",
    "    \n",
    "    ax[0].plot(losses)\n",
    "    ax[0].set_title('Epoch={}, lr={:.4f}, loss={:.4f}, pixel_loss={:.4f}'.format(e, lr, losses[-1], losses_dict['pixel']))\n",
    "    \n",
    "    ax[1].imshow(image1, cmap='gray', vmin=0, vmax=255)\n",
    "    ax[1].invert_yaxis()\n",
    "    ax[1].set_title('Current image')\n",
    "    \n",
    "    ax[2].imshow(image2, cmap='gray', vmin=0, vmax=255)\n",
    "    ax[2].invert_yaxis()\n",
    "    ax[2].set_title('Target image')\n",
    "\n",
    "    ax[3].imshow(image3, cmap='gray', vmin=0, vmax=1.0)\n",
    "    ax[3].set_title('Learned texturemap')\n",
    "\n",
    "    \n",
    "    plt.savefig(out_path, dpi=300)\n",
    "    plt.close('all')\n",
    "    \n",
    "    saved_img = imageio.imread(save_path)\n",
    "    return saved_img\n",
    "    \n",
    "def get_gpu_stats(output_str=True):\n",
    "    mb_reserved = torch.cuda.memory_reserved() * 0.000001\n",
    "    mb_alloc = torch.cuda.memory_allocated() * 0.000001\n",
    "    mb_cached = torch.cuda.memory_cached() * 0.000001\n",
    "    if output_str:\n",
    "        return 'alloc={:,.0f}MB | cached={:,.0f}MB | reserved={:,.0f}MB'.format(mb_alloc, mb_cached, mb_reserved)\n",
    "    else:\n",
    "        return mb_alloc, mb_cached, mb_reserved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = r'./7_data/output/plot.png'\n",
    "# saved_img = visualize_LR(0, 0, [100, 10, 1], images_rendered, texture_map, save_path)\n",
    "i = 0\n",
    "image1 = images_rendered.squeeze().detach().cpu().numpy()[i]\n",
    "image2 = img_refs[i]\n",
    "image3 = texture_map.detach().squeeze().cpu().numpy()\n",
    "saved_img = visualize2(1, 2, [3, 4, 5], image1, image2, image3, save_path)\n",
    "plt.figure(figsize=(24, 6))\n",
    "plt.imshow(saved_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init mesh & texturemap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- torch.cuda.empty_cache() -----\n",
      "torch.cuda.memory_allocated(): 1.99 Mb\n",
      "torch.cuda.memory_cached(): 6.29 Mb\n",
      "  - batch size: 1, num_batches: 16\n",
      "  - cameras   : 16\n",
      "  - renderers   : 16\n"
     ]
    }
   ],
   "source": [
    "print('----- torch.cuda.empty_cache() -----')\n",
    "torch.cuda.empty_cache()\n",
    "bytes_alloc = torch.cuda.memory_allocated()\n",
    "print('torch.cuda.memory_allocated(): {:,.2f} Mb'.format(bytes_alloc * 0.000001))\n",
    "bytes_cached = torch.cuda.memory_cached()\n",
    "print('torch.cuda.memory_cached(): {:,.2f} Mb'.format(bytes_cached * 0.000001))\n",
    "\n",
    "model = Model(device_gpu, img_dir=img_dir, cam_params=cams_torch, image_refs=img_refs, texturemap_shape=texturemap_shape, mesh_path=mesh_path, image_size=image_size, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, images, texels, stat_gpu, losses_dict, _ = model()\n",
    "i = 0\n",
    "save_path = r'./7_data/output/plot.png'\n",
    "image1 = images.squeeze().detach().cpu().numpy()[i]\n",
    "image2 = img_refs[i]\n",
    "image3 = texels.detach().squeeze().cpu().numpy()\n",
    "saved_img = visualize2(1, 2, [3, 4, 5], losses_dict, image1, image2, image3, save_path)\n",
    "plt.figure(figsize=(40, 10))\n",
    "plt.imshow(saved_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log output: ./10b_data/output/log_20200415_02h10m.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-67bd7d3022fe>:17: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  loop = tqdm_notebook(range(1000000000))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f6f2b3ca4b949f2957033392ced0a63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1000000000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32, 4])\n",
      "torch.Size([1, 32, 32])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-67bd7d3022fe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     41\u001b[0m             \u001b[0mlatest_states\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss_dict'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstat_gpu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m             \u001b[0mlatest_states\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[0mlatest_states\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss_dict'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\pytorch3d\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-856acde8dac3>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m             \u001b[1;31m# [0.0, 1.0] float32 in gpu -> change to [0.0, 255.0] cpu. shape (batch_size, W, H, 4)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m             \u001b[0mimage_cur\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_renderers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmeshes_world\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmeshes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtexture_maps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtexture_map\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m             \u001b[1;31m# save for output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[0mimages_out\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mi1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage_cur\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\pytorch3d\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\1_projects\\200323_pytorch3dstart\\pytorch3d\\renderer\\mesh\\renderer.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, meshes_world, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m             )\n\u001b[0;32m     75\u001b[0m         \u001b[0mt2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m         \u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_shader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfragments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeshes_world\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m         \u001b[0mt3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[0mlog\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m'{:<10}: {:.3f}s\\n'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'renderer'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt3\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mt0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\pytorch3d\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\1_projects\\200323_pytorch3dstart\\pytorch3d\\renderer\\mesh\\shader.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, fragments, meshes, **kwargs)\u001b[0m\n\u001b[0;32m    270\u001b[0m         \u001b[0mt0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    271\u001b[0m         \u001b[0mtexture_maps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'texture_maps'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 272\u001b[1;33m         \u001b[0mtexels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_texturemap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minterpolate_texture_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfragments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeshes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtexture_maps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtexture_maps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    273\u001b[0m         \u001b[0mt1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\1_projects\\200323_pytorch3dstart\\pytorch3d\\renderer\\mesh\\texturing.py\u001b[0m in \u001b[0;36minterpolate_texture_map\u001b[1;34m(fragments, meshes, texture_maps)\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[0mt0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[1;31m# pixel_uvs: (N, H, W, K, 2)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m     pixel_uvs, log_face = interpolate_face_attributes(\n\u001b[0m\u001b[0;32m     53\u001b[0m         \u001b[0mfragments\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpix_to_face\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfragments\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbary_coords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfaces_verts_uvs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     )\n",
      "\u001b[1;32md:\\1_projects\\200323_pytorch3dstart\\pytorch3d\\renderer\\mesh\\utils.py\u001b[0m in \u001b[0;36minterpolate_face_attributes\u001b[1;34m(pix_to_face, barycentric_coords, face_attributes)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[0mpix_to_face\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpix_to_face\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[0mt2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m     \u001b[0mpix_to_face\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[0midx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpix_to_face\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mH\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mW\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mH\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mW\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mD\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[0mt3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "out_dir = './10b_data/output'\n",
    "save_path = out_dir + '/plot.png'\n",
    "\n",
    "# filename_output = './4_data/output/4b/out.gif'\n",
    "# writer = imageio.get_writer(filename_output, mode='I', duration=0.3)\n",
    "now = datetime.now()\n",
    "hour = str(now.hour)\n",
    "minute = str(now.minute)\n",
    "date_str = '{}{:>02}{:>02}_{:>02}h{:>02}m'.format(now.year, now.month, now.day, hour, minute)\n",
    "log_path = out_dir + '/log_{}.txt'.format(date_str)\n",
    "__output_log(log_path, '========== {} Start ==========================\\n'.format(date_str))\n",
    "print('Log output: {}'.format(log_path))\n",
    "\n",
    "lr = 0.1\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "loop = tqdm_notebook(range(1000000000))\n",
    "\n",
    "losses = []\n",
    "loss_belows, loss_aboves = 0, 0\n",
    "losses_below, losses_above = [], []\n",
    "latest_states = {}\n",
    "with torch.autograd.profiler.profile() as prof:\n",
    "    for e in loop:\n",
    "        stat_str = '1 {:<21}: {}\\n'.format('epoch starts'.format(e), get_gpu_stats())\n",
    "        output_texels = (e % 10 == 0)\n",
    "        \n",
    "        if e > 0 and e % 200 == 0:\n",
    "            optimizer.param_groups[0]['lr'] *= 0.5\n",
    "            \n",
    "        t0 = time.time()\n",
    "        optimizer.zero_grad()\n",
    "        stat_str += '2 {:<21}: {}\\n'.format('optimizer.zero_grad()', get_gpu_stats())\n",
    "        t1 = time.time()\n",
    "        \n",
    "        if output_texels:\n",
    "            loss, images, texels, stat_gpu, loss_dict, _ = model()\n",
    "            latest_states['loss'] = loss\n",
    "            latest_states['images'] = images.clone()\n",
    "            latest_states['texels'] = texels.clone()\n",
    "            latest_states['loss_dict'] = loss_dict\n",
    "        else:\n",
    "            loss, _, _, stat_gpu, loss_dict, _ = model()\n",
    "            latest_states['loss'] = loss\n",
    "            latest_states['loss_dict'] = loss_dict\n",
    "            \n",
    "        if e > 0 and e % 50 == 0:\n",
    "            model.save_parameters('./7_data/output/deform_verts.npy')\n",
    "            model.export_obj('./7_data/output/obj.obj')\n",
    "            \n",
    "        stat_str += '3 {:<21}: {}\\n'.format('forward', get_gpu_stats())\n",
    "        stat_str += stat_gpu\n",
    "        t2 = time.time()\n",
    "        loss.backward()\n",
    "        stat_str += '4 {:<21}: {}\\n'.format('backward', get_gpu_stats())\n",
    "        t3 = time.time()\n",
    "        \n",
    "        optimizer.step()\n",
    "        stat_str += '5 {:<21}: {}\\n'.format('optimizer.step()', get_gpu_stats())\n",
    "        t4 = time.time()\n",
    "        losses.append(loss)\n",
    "        losses_below.append(loss_belows)\n",
    "        losses_above.append(loss_aboves)\n",
    "\n",
    "        loop.set_description('[{}/{}] loss={:.6f}'.format(e, len(loop), loss.data))\n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        if output_texels:\n",
    "            # Save outputs to create a GIF. \n",
    "            t10 = time.time()\n",
    "            \n",
    "            save_idx = 0\n",
    "            image1 = images.squeeze().detach().cpu().numpy()[save_idx]\n",
    "            image2 = img_refs[save_idx]\n",
    "            image3 = texels.detach().squeeze().cpu().numpy()\n",
    "            saved_img = visualize2(e, lr, losses, loss_dict, image1, image2, image3, save_path)\n",
    "            # image_out = visualize_LR(e, lr, losses, images, texels, save_path)\n",
    "            plt.figure()\n",
    "            plt.imshow(saved_img)\n",
    "            plt.title(\"iter: %d, lr: %0.4f, loss: %0.8f\" % (e, lr, loss.data))\n",
    "            plt.grid(\"off\")\n",
    "            plt.axis(\"off\")\n",
    "            plt.show()\n",
    "            plt.close('all')\n",
    "            \n",
    "            texels_np = model.texture_map.detach().cpu().numpy()\n",
    "            np.save(out_dir + '/texturemap.npy', texels_np)\n",
    "            del texels_np\n",
    "            \n",
    "            texturemap_out = (255.0*np.clip(model.texture_map.detach().squeeze().cpu().numpy(), a_min=0, a_max=1.0)).astype(np.uint8)\n",
    "            im = Image.fromarray(texturemap_out)\n",
    "            im.save(out_dir + '/texturemap_learned.png', dpi=(600, 600))\n",
    "            t11 = time.time()\n",
    "\n",
    "#             image_out = image_out / 255.0\n",
    "#             image_out = np.clip(image_out, 0, 1)\n",
    "#             image_out = img_as_ubyte(image_out)\n",
    "#             writer.append_data(image_out)\n",
    "            t12 = time.time()\n",
    "            \n",
    "            __output_log(log_path, '{:03} | plot({:.2f}s) | gif({:.2f}s)\\n'.format(e+1, t11-t10, t12-t11))\n",
    "#             stat_str += '  {:<21}: {}\\n'.format('plotting', get_gpu_stats())\n",
    "        # execution time\n",
    "        t01 = t1-t0\n",
    "        t12 = t2-t1\n",
    "        t23 = t3-t2\n",
    "        t34 = t4-t3\n",
    "        t5 = time.time()\n",
    "        \n",
    "        mb_alloc = torch.cuda.memory_allocated() * 0.000001\n",
    "        mb_cached = torch.cuda.memory_cached() * 0.000001\n",
    "        now = datetime.now()\n",
    "        hour = str(now.hour)\n",
    "        minute = str(now.minute)\n",
    "        second = str(now.second)\n",
    "        now_str = '{:>02}:{:>02}:{:>02}'.format(hour, minute, second)\n",
    "        __output_log(log_path, '{} | {:03} | {:.2f}s | lr={:.8f} | loss={:.6f} | pixel_l={:.4f} | normal_l={:.4f} | lap_l={:.4f} | zero_grad({:.2f}s) | forward({:.2f}s) | backward({:.2f}s) | step({:.2f}s) | GPU_allocated({:,.2f}Mb) | GPU_cached({:,.2f}Mb)\\n'.format(now_str, e+1, t5-t0, lr, loss, loss_dict['pixel'], loss_dict['mesh_normal'], loss_dict['mesh_laplacian'], t01, t12, t23, t34, mb_alloc, mb_cached))\n",
    "        \n",
    "        # clean up\n",
    "        if output_texels:\n",
    "            del loss, images, texels\n",
    "        else:\n",
    "            del loss\n",
    "        stat_str += '7 {:<21}: {}\\n'.format('clean up', get_gpu_stats())\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        stat_str += '8 {:<21}: {}\\n'.format('empty_cache()', get_gpu_stats())\n",
    "        __output_log(log_path, '{}'.format(stat_str))\n",
    "        if e > 620:\n",
    "            break\n",
    "            \n",
    "torch.cuda.empty_cache()\n",
    "# loss, images, texels, stat_str = model()\n",
    "loss, images, texels = latest_states['loss'], latest_states['images'], latest_states['texels']\n",
    "print(stat_str)\n",
    "\n",
    "save_dir = out_dir\n",
    "for i in range(len(images)):\n",
    "    print(' ', i+1, end='')\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(24, 12), tight_layout=True)\n",
    "    img = images[i].detach().cpu().numpy()\n",
    "    ax[0].imshow(img, cmap='gray', vmin=0, vmax=255)\n",
    "    ax[0].invert_yaxis()\n",
    "    ax[1].imshow(img_refs[i], cmap='gray')\n",
    "    ax[1].invert_yaxis()\n",
    "    plt.savefig(save_dir + '/compare_cam{}.png'.format(i+1), dpi=300)\n",
    "print(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure()\n",
    "# # plt.plot(losses, 'k')\n",
    "# plt.plot(losses_above, 'r')\n",
    "# plt.plot(losses_below, 'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "# loss, images, texels, stat_str = model()\n",
    "loss, images, texels = latest_states['loss'], latest_states['images'], latest_states['texels']\n",
    "print(stat_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "texture_maps = texels.detach().squeeze().cpu()\n",
    "td = texels.detach().cpu()\n",
    "print(td.shape)\n",
    "print(torch.min(td),',', torch.max(td))\n",
    "\n",
    "td_a = torch.clamp(-1.0*texture_maps, min=0.0)\n",
    "print('n_above:', torch.sum(td_a > 0.0))\n",
    "td_b = torch.clamp(texture_maps, min=1.0) - torch.ones(texture_maps.shape)\n",
    "print('n_below:', torch.sum(td_b > 1.0))\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20, 10))\n",
    "ax[0].imshow(td_a, vmin=0, vmax=1.0)\n",
    "ax[0].set_title('Pixels > 1.0')\n",
    "ax[1].imshow(td_b, vmin=0, vmax=1.0)\n",
    "ax[1].set_title('Pixels < 0.0')\n",
    "plt.figure()\n",
    "plt.imshow(td.clone().squeeze().numpy(), cmap='gray', vmin=0, vmax=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save rendered images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "save_dir = out_dir\n",
    "for i in range(len(images)):\n",
    "    print(' ', i+1, end='')\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(24, 12), tight_layout=True)\n",
    "    img = images[i].detach().cpu().numpy()\n",
    "    ax[0].imshow(img, cmap='gray', vmin=0, vmax=255)\n",
    "    ax[0].invert_yaxis()\n",
    "    ax[1].imshow(img_refs[i], cmap='gray')\n",
    "    ax[1].invert_yaxis()\n",
    "    plt.savefig(save_dir + '/compare_cam{}.png'.format(i+1), dpi=300)\n",
    "print(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "fig, ax = plt.subplots(16, 1, figsize=(15, 140))\n",
    "ax = ax.ravel()\n",
    "for i in range(len(ax)):\n",
    "    img_mesh = images_rendered[i]\n",
    "    img_mesh = cv2.flip(img_mesh, -1)\n",
    "    img_bg = image_refs[i]\n",
    "    img_bg = cv2.flip(img_bg, -1)\n",
    "    ax[i].imshow(img_bg)\n",
    "\n",
    "    img_mesh_large = np.zeros(img_bg.shape)\n",
    "    img_mesh_padded = cv2.copyMakeBorder(img_mesh, 0, 0, int((4000-2160)/2), int((4000-2160)/2), 0, None, [0, 0, 0])\n",
    "    ax[i].imshow(img_mesh, alpha=0.5)\n",
    "    \n",
    "    pts = mesh_points[i]\n",
    "    pts_small_x = (pts[:, 0] - (4000-2160)*0.5) * rendered_image_size/2160\n",
    "    pts_small_y = pts[:, 1] * rendered_image_size/2160\n",
    "    pts_small = np.stack([pts_small_x, pts_small_y]).T\n",
    "    pts_center = np.mean(pts_small, axis=0)\n",
    "#     ax[i].scatter(pts[:, 0], pts[:, 1], c='r', s=0.1)\n",
    "#     ax[i].scatter(pts_small[:, 0], pts_small[:, 1], c='r', s=0.01)\n",
    "    ax[i].set_title('Camera {}'.format(cams[i]))\n",
    "    \n",
    "    # plot centers\n",
    "    ax[i].scatter(pts_center[0], pts_center[1], c='r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, meshes, image_refs, renderers, texture_maps):\n",
    "        super().__init__()\n",
    "        self.meshes = meshes\n",
    "        self.device = meshes.device\n",
    "        self.renderers = renderers\n",
    "        self.register_buffer('image_refs', image_refs)\n",
    "\n",
    "        self.texture_maps = nn.Parameter(texture_maps.to(meshes.device), requires_grad=True)\n",
    "        \n",
    "    def forward(self):\n",
    "        loss = 0\n",
    "        images = []\n",
    "        for i in range(len(self.renderers)):\n",
    "            image = self.renderers[i](meshes_world=self.meshes, texture_maps=self.texture_maps)\n",
    "            loss_i = torch.mean((image.squeeze()[..., :3] - self.image_refs[i]) ** 2)\n",
    "            images.append(image)\n",
    "            loss = loss + loss_i\n",
    "        loss /= len(self.renderers)\n",
    "\n",
    "        return loss, images, self.texture_maps.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After traning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(model.texture_maps.detach().cpu().numpy()[0, :, :, :])\n",
    "plt.savefig('./4_data/output/4b/texturemap_learned.png', dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn((1, 1), requires_grad=True)\n",
    "with torch.autograd.profiler.profile() as prof:\n",
    "    print(prof)\n",
    "    for _ in range(100):  # any normal python code, really!\n",
    "        y = x ** 2\n",
    "        y.backward()\n",
    "# NOTE: some columns were removed for brevity\n",
    "print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.memory_stats(device=device_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.memory_summary(device=device_gpu, abbreviated=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_stats = torch.cuda.memory_stats(device=device_gpu)\n",
    "for k, v in mem_stats.items():\n",
    "    print('{}: {}'.format(k, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.max_memory_allocated(device=device_gpu))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
