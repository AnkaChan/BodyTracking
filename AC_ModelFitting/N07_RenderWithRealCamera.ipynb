{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:96% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.1\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:96% !important; }</style>\"))\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.io import imread\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from skimage import img_as_ubyte\n",
    "import imageio\n",
    "import json\n",
    "import cv2\n",
    "import time\n",
    "from PIL import Image\n",
    "from pytorch3d.loss import (\n",
    "    mesh_laplacian_smoothing, \n",
    "    mesh_normal_consistency,\n",
    ")\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import Utility\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "# Util function for loading meshes\n",
    "from pytorch3d.io import load_objs_as_meshes, load_obj, load_ply\n",
    "import math\n",
    "# Data structures and functions for rendering\n",
    "from pytorch3d.structures import Meshes, Textures, join_meshes_as_batch\n",
    "from pytorch3d.renderer import (\n",
    "    look_at_view_transform,\n",
    "    OpenGLPerspectiveCameras, \n",
    "    SfMPerspectiveCameras,\n",
    "    SfMOrthographicCameras,\n",
    "    PointLights, \n",
    "    BlendParams,\n",
    "    DirectionalLights,\n",
    "    Materials, \n",
    "    RasterizationSettings, \n",
    "    MeshRenderer, \n",
    "    MeshRasterizer,  \n",
    "    TexturedSoftPhongShader,\n",
    "    SoftSilhouetteShader,\n",
    "    look_at_rotation,\n",
    "    HardFlatShader\n",
    ")\n",
    "\n",
    "# add path for demo utils functions \n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "sys.path.append(os.path.abspath(''))\n",
    "\n",
    "print(torch.version.cuda)\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(Utility)\n",
    "from Utility import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "camParamF = r'F:\\WorkingCopy2\\2020_05_31_DifferentiableRendererRealData\\CameraParams\\cam_params.json'\n",
    "imageFolder = r'F:\\WorkingCopy2\\2020_05_31_DifferentiableRendererRealData\\Images\\03052\\Undist'\n",
    "modelFile = r'F:\\WorkingCopy2\\2020_05_31_DifferentiableRendererRealData\\Models\\03052.obj'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_img_shape: (2160, 4000)\n",
      "16 : dict_keys(['K', 'dist', 'R', 'T', 'fx', 'fy', 'cx', 'cy'])\n",
      "dict_keys(['R', 'T', 'fl', 'pp'])\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "actual_img_shape = (2160, 4000)\n",
    "cam_params, cams_torch = load_cameras(camParamF, device, actual_img_shape)\n",
    "print(len(cam_params), ':', cam_params[0].keys())\n",
    "print(cams_torch.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_refs_out, crops_out = load_images(imageFolder, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2160, 4000, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_refs_out[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize2DResults(crops_out, pytorch3DImg=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cams_torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cams = init_camera_batches(cams_torch, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verts, faces, aux = load_obj(modelFile)\n",
    "faces_idx = faces.verts_idx\n",
    "\n",
    "mesh = Meshes(\n",
    "                verts=[verts.to(device)],   \n",
    "                faces=[faces_idx.to(device)], \n",
    "#                 textures=textures.to(device)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RenderingCfg:\n",
    "    def __init__(s):\n",
    "        s.sigma = 1e-4\n",
    "        s.blurRange = 1e-4\n",
    "        s.faces_per_pixel = 50\n",
    "        s.bodyJointOnly = False\n",
    "        s.randSeedPerturb = 1234\n",
    "        s.noiseLevel = 0.5\n",
    "        s.numIterations = 2000\n",
    "        s.learningRate = 0.005\n",
    "        s.terminateLoss = 200\n",
    "        s.plotStep = 10\n",
    "        s.numCams = 8\n",
    "        s.imgSize = 2160\n",
    "        \n",
    "class Renderer:\n",
    "    def __init__(s, cfg = RenderingCfg):\n",
    "        s.cfg = cfg\n",
    "        # blend_params = BlendParams(sigma=1e-4, gamma=1e-4)\n",
    "        s.blend_params = BlendParams(sigma=cfg.sigma, gamma=1e-4)\n",
    "\n",
    "        # Place a point light in front of the object. As mentioned above, the front of the cow is facing the \n",
    "        # -z direction. \n",
    "        s.lights = PointLights(device=device, location=[[0.0, 0.0, -3.0]])\n",
    "#         cameras = OpenGLPerspectiveCameras(device=device)\n",
    "        # Create a phong renderer by composing a rasterizer and a shader. The textured phong shader will \n",
    "        # interpolate the texture uv coordinates for each vertex, sample from a texture image and \n",
    "        # apply the Phong lighting model\n",
    "        \n",
    "        if cfg.blurRange!= 0:\n",
    "            s.raster_settings = RasterizationSettings(\n",
    "                image_size=cfg.imgSize, \n",
    "                blur_radius= np.log(1. / cfg.blurRange - 1.) * s.blend_params.sigma, \n",
    "                faces_per_pixel=cfg.faces_per_pixel, \n",
    "                bin_size=0\n",
    "            )\n",
    "        else:\n",
    "            s.raster_settings = RasterizationSettings(\n",
    "                image_size=cfg.imgSize, \n",
    "                blur_radius= 0, \n",
    "                faces_per_pixel=cfg.faces_per_pixel, \n",
    "                bin_size=0\n",
    "            )\n",
    "            \n",
    "        s.rasterizer=MeshRasterizer(\n",
    "                cameras=None, \n",
    "                raster_settings=s.raster_settings\n",
    "            )\n",
    "        if cfg.blurRange!= 0:\n",
    "            s.renderer = MeshRenderer(\n",
    "                rasterizer = s.rasterizer,\n",
    "            #     shader=SoftPhongShader(\n",
    "            #         device=device, \n",
    "            #         cameras=cameras,\n",
    "            #         lights=lights,\n",
    "            #         blend_params=blend_params\n",
    "            #     )\n",
    "                shader=SoftSilhouetteShader(\n",
    "                    blend_params=s.blend_params\n",
    "                    # device=device, \n",
    "                    # cameras=cameras,\n",
    "                    # lights=lights\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            s.renderer = MeshRenderer(\n",
    "                rasterizer = s.rasterizer,\n",
    "            #     shader=SoftPhongShader(\n",
    "            #         device=device, \n",
    "            #         cameras=cameras,\n",
    "            #         lights=lights,\n",
    "            #         blend_params=blend_params\n",
    "            #     )\n",
    "                shader=SoftSilhouetteShader(\n",
    "                    blend_params=s.blend_params\n",
    "                    # device=device, \n",
    "                    # cameras=cameras,\n",
    "                    # lights=lights\n",
    "                )\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfgRef = RenderingCfg()\n",
    "cfgRef.faces_per_pixel = 1\n",
    "\n",
    "# cfgRef.blurRange = 1e-1\n",
    "cfgRef.blurRange = 0\n",
    "# cfgRef.imgSize = 2000\n",
    "\n",
    "renderRef = Renderer(cfgRef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = RenderingCfg()\n",
    "# cfg.sigma = 1e-3\n",
    "cfg.noiseLevel = 0.1\n",
    "# cfg.blurRange = 1e-1\n",
    "cfg.sigma = 1e-5\n",
    "cfg.blurRange = 1e-4\n",
    "cfg.plotStep = 5\n",
    "cfg.learningRate = 0.001\n",
    "\n",
    "cfg.faces_per_pixel = 15\n",
    "cfg.imgSize = 2160     \n",
    "renderSynth = Renderer(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "for iCam in range(len(cams)):\n",
    "    image_cur = renderRef.renderer(mesh,  cameras=cams[iCam])\n",
    "    images.append(image_cur.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize2DResults(images, backGroundImages=None, outImgFile=None, rows = 2, pytorch3DImg=True):\n",
    "    lossVal = 0\n",
    "    numCams = len(images)\n",
    "    numCols = int(numCams / rows)\n",
    "    fig, axs = plt.subplots(rows, numCols)\n",
    "    fig.set_size_inches(numCols*2, rows*2)\n",
    "    with torch.no_grad():\n",
    "        for iRow in range(rows):\n",
    "            for iCol in range(numCols):\n",
    "                iCam = rows* iRow + iCol\n",
    "                if pytorch3DImg:\n",
    "                    imgAlpha = images[iCam][0,...,3]\n",
    "                else:\n",
    "                    imgAlpha = images[iCam]\n",
    "                    \n",
    "                if backGroundImages is not None:\n",
    "                    img = np.copy(backGroundImages[iCam])\n",
    "#                     fgMask = np.logical_not(np.where())\n",
    "                    for iChannel in range(3):\n",
    "                        img[..., iChannel] = np.where(imgAlpha, imgAlpha, backGroundImages[iCam][...,iChannel])\n",
    "                    imgAlpha = img\n",
    "                    \n",
    "                imgAlpha = cv2.flip(imgAlpha, -1)\n",
    "                \n",
    "                axs[iRow, iCol].imshow(imgAlpha, vmin=0.0, vmax=1.0)\n",
    "                axs[iRow, iCol].axis('off')\n",
    "\n",
    "        if outImgFile is not None:\n",
    "            plt.savefig(outImgFile, dpi=512, transparent=True, bbox_inches='tight', pad_inches=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize2DResults(images, backGroundImages=crops_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
