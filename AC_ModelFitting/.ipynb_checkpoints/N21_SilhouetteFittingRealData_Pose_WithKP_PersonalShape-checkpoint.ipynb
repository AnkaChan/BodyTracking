{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:96% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:96% !important; }</style>\"))\n",
    "\n",
    "SMPLSH_Dir = r'..\\SMPL_reimp'\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, SMPLSH_Dir)\n",
    "import smplsh_torch\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.io import imread\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from skimage import img_as_ubyte\n",
    "import imageio\n",
    "import json\n",
    "import cv2\n",
    "import time\n",
    "from PIL import Image\n",
    "from pytorch3d.loss import (\n",
    "    mesh_laplacian_smoothing, \n",
    "    mesh_normal_consistency,\n",
    ")\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import Utility\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "# Util function for loading meshes\n",
    "from pytorch3d.io import load_objs_as_meshes, load_obj, load_ply\n",
    "import math\n",
    "# Data structures and functions for rendering\n",
    "from pytorch3d.structures import Meshes, Textures, join_meshes_as_batch\n",
    "from pytorch3d.renderer import (\n",
    "    look_at_view_transform,\n",
    "    OpenGLPerspectiveCameras, \n",
    "    SfMPerspectiveCameras,\n",
    "    SfMOrthographicCameras,\n",
    "    PointLights, \n",
    "    BlendParams,\n",
    "    DirectionalLights,\n",
    "    Materials, \n",
    "    RasterizationSettings, \n",
    "    MeshRenderer, \n",
    "    MeshRasterizer,  \n",
    "    TexturedSoftPhongShader,\n",
    "    SoftSilhouetteShader,\n",
    "    look_at_rotation,\n",
    "    HardFlatShader\n",
    ")\n",
    "from pytorch3d.transforms.so3 import (\n",
    "    so3_exponential_map,\n",
    "    so3_relative_angle,\n",
    ")\n",
    "# add path for demo utils functions \n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(''))\n",
    "import json\n",
    "from os.path import join\n",
    "\n",
    "import pyvista as pv\n",
    "\n",
    "import Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(Utility)\n",
    "from Utility import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def renderMesh(camRTs, cfg, mesh, renderer):\n",
    "    images= []\n",
    "    with torch.no_grad():\n",
    "        for iCam in range(cfg.numCams):\n",
    "            R=camRTs[iCam]['R']\n",
    "            T=camRTs[iCam]['T']\n",
    "            image = renderer.renderer(meshes_world=mesh, R=R, T=T).cpu().numpy()\n",
    "            images.append(image)\n",
    "    return images\n",
    "\n",
    "def saveVTK(outFile, verts, smplshExampleMesh):\n",
    "    smplshExampleMesh.points = verts\n",
    "    smplshExampleMesh.save(outFile)\n",
    "\n",
    "def visualize2DSilhouetteResults(images, backGroundImages=None, outImgFile=None, rows = 2, pytorch3DImg=True, sizeInInches = 2):\n",
    "    lossVal = 0\n",
    "    numCams = len(images)\n",
    "    numCols = int(numCams / rows)\n",
    "    fig, axs = plt.subplots(rows, numCols)\n",
    "    fig.set_size_inches(numCols*sizeInInches, rows*sizeInInches)\n",
    "    with torch.no_grad():\n",
    "        for iRow in range(rows):\n",
    "            for iCol in range(numCols):\n",
    "                iCam = rows* iRow + iCol\n",
    "                imgAlpha = images[iCam][0,...,3]\n",
    "                    \n",
    "                if backGroundImages is not None:\n",
    "                    img = np.copy(backGroundImages[iCam]) * 0.5\n",
    "#                     fgMask = np.logical_not(np.where())\n",
    "#                     for iChannel in range(3):\n",
    "                    img[..., 0] = img[..., 0] + imgAlpha * 0.5\n",
    "                    imgAlpha = img\n",
    "                    \n",
    "                imgAlpha = cv2.flip(imgAlpha, -1)\n",
    "                \n",
    "                axs[iRow, iCol].imshow(imgAlpha, vmin=0.0, vmax=1.0)\n",
    "                axs[iRow, iCol].axis('off')\n",
    "\n",
    "        if outImgFile is not None:\n",
    "            fig.savefig(outImgFile, dpi=512, transparent=True, bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "outFolder = r'F:\\WorkingCopy2\\2020_05_31_DifferentiableRendererRealData\\Output\\03067\\RealDataPose'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "camParamF = r'F:\\WorkingCopy2\\2020_05_31_DifferentiableRendererRealData\\CameraParams\\cam_params.json'\n",
    "imageFolder = r'F:\\WorkingCopy2\\2020_06_04_SilhouetteExtraction\\03067\\silhouettes'\n",
    "\n",
    "# modelFile = r'F:\\WorkingCopy2\\2020_05_31_DifferentiableRendererRealData\\Models\\03052.obj'\n",
    "modelFile = r'F:\\WorkingCopy2\\2020_06_14_FitToMultipleCams\\FitToSparseCloud\\03067.obj'\n",
    "\n",
    "smplshExampleMeshFile = r'C:\\Code\\MyRepo\\ChbCapture\\06_Deformation\\SMPL_Socks\\SMPLSH\\SMPLSH.obj'\n",
    "\n",
    "KeypointsFile = r'F:\\WorkingCopy2\\2020_06_14_FitToMultipleCams\\KepPoints\\03067.obj'\n",
    "# initialFittingParamFile = r'F:\\WorkingCopy2\\2020_05_21_AC_FramesDataToFitTo\\FitToSparseCloud\\FittingParams\\03052.npz'\n",
    "smplshRegressorMatFile = r'C:\\Code\\MyRepo\\ChbCapture\\08_CNNs\\Openpose\\SMPLSHAlignToAdamWithHeadNoFemurHead\\smplshRegressorNoFlatten.npy'\n",
    "\n",
    "# compressedStorage = False\n",
    "# initialFittingParamPoseFile = r'C:\\Code\\MyRepo\\ChbCapture\\06_Deformation\\SMPL_Socks\\SMPLSHFit\\LadaOldSuit_WithOPKeypoints_DenseSparsePts\\OptimizedPoses_ICPTriangle.npy'\n",
    "# initialFittingParamBetasFile = r'C:\\Code\\MyRepo\\ChbCapture\\06_Deformation\\SMPL_Socks\\SMPLSHFit\\LadaOldSuit_WithOPKeypoints_DenseSparsePts\\OptimizedBetas_ICPTriangle.npy'\n",
    "# initialFittingParamTranslationFile = r'C:\\Code\\MyRepo\\ChbCapture\\06_Deformation\\SMPL_Socks\\SMPLSHFit\\LadaOldSuit_WithOPKeypoints_DenseSparsePts\\OptimizedTranslation_ICPTriangle.npy'\n",
    "\n",
    "compressedStorage = True\n",
    "initialFittingParamFile = r'F:\\WorkingCopy2\\2020_06_14_FitToMultipleCams\\FitToSparseCloud\\FittingParams\\03067.npz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 6750)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smplshData = r'C:\\Code\\MyRepo\\ChbCapture\\06_Deformation\\SMPL_Socks\\SMPLSH\\SmplshModel.npz'\n",
    "\n",
    "smplshExampleMeshFile = r'C:\\Code\\MyRepo\\ChbCapture\\06_Deformation\\SMPL_Socks\\SMPLSH\\SMPLSH.obj'\n",
    "# Setup\n",
    "device = torch.device(\"cuda:0\")\n",
    "torch.cuda.set_device(device)\n",
    "\n",
    "pose_size = 3 * 52\n",
    "beta_size = 10\n",
    "OPHeadKeypoints = [0, 15, 16, 17, 18]\n",
    "\n",
    "smplshExampleMesh = pv.PolyData(smplshExampleMeshFile)\n",
    "# The head joint regressor\n",
    "Keypoints = pv.PolyData(KeypointsFile)\n",
    "\n",
    "smplshRegressorMat = np.load(smplshRegressorMatFile)\n",
    "smplshRegressorMatHead = smplshRegressorMat[-5:, :]\n",
    "smplshRegressorMatHead.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if compressedStorage:\n",
    "    fitParam = np.load(initialFittingParamFile)\n",
    "    transInit = fitParam['trans']\n",
    "    poseInit = fitParam['pose']\n",
    "    betaInit = fitParam['beta']\n",
    "else:\n",
    "    transInit = np.load(initialFittingParamTranslationFile)\n",
    "    poseInit = np.load(initialFittingParamPoseFile)\n",
    "    betaInit = np.load(initialFittingParamBetasFile)\n",
    "    \n",
    "smplsh = smplsh_torch.SMPLModel(device, smplshData)\n",
    "\n",
    "pose = torch.tensor(poseInit, dtype=torch.float64, requires_grad = True, device=device)\n",
    "betas = torch.tensor(betaInit, dtype=torch.float64, requires_grad = True, device=device)\n",
    "trans = torch.tensor(transInit, dtype=torch.float64, \n",
    "                     requires_grad = True, device=device)\n",
    "\n",
    "verts = smplsh(betas, pose, trans).type(torch.float32) * 1000\n",
    "\n",
    "smplshMesh = Meshes([verts], [smplsh.faces.to(device)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyvista_ndarray([[-7.8175354 , 16.44110107, -6.62634277],\n",
       "                 [ 7.90048218,  0.3745575 ,  6.59020996],\n",
       "                 [ 8.01235962, -4.58454895,  9.14575195],\n",
       "                 [-7.56030273, -4.96923828, -4.0324707 ],\n",
       "                 [-2.00279236, -9.49133301, -0.88757324]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validate the head points regressor\n",
    "headJoints = smplshRegressorMatHead @ verts.cpu().detach().numpy()\n",
    "headJoints - Keypoints.points[OPHeadKeypoints, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_img_shape: (2160, 4000)\n"
     ]
    }
   ],
   "source": [
    "actual_img_shape = (2160, 4000)\n",
    "cam_params, cams_torch = load_cameras(camParamF, device, actual_img_shape)\n",
    "cams = init_camera_batches(cams_torch, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'img' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-a385a5975410>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# load Images\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mimage_refs_out\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcrops_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_images\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimageFolder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcropSize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1080\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Code\\MyRepo\\03_capture\\BodyTracking\\AC_ModelFitting\\Utility.py\u001b[0m in \u001b[0;36mload_images\u001b[1;34m(img_dir, UndistImgs, cropSize, imgExt)\u001b[0m\n\u001b[0;32m    156\u001b[0m         \u001b[0mimage_refs_out\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 158\u001b[1;33m     \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_refs_out\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'img' referenced before assignment"
     ]
    }
   ],
   "source": [
    "# load Images\n",
    "image_refs_out, crops_out = load_images(imageFolder, cropSize=1080)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crops_out[0].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RenderingCfg:\n",
    "    def __init__(s):\n",
    "        s.sigma = 1e-4\n",
    "        s.blurRange = 1e-4\n",
    "        s.faces_per_pixel = 50\n",
    "        s.bodyJointOnly = False\n",
    "        s.randSeedPerturb = 1234\n",
    "        s.noiseLevel = 0.5\n",
    "        s.numIterations = 2000\n",
    "        s.learningRate = 0.005\n",
    "        s.terminateLoss = 200\n",
    "        s.plotStep = 10\n",
    "        s.numCams = 16\n",
    "        s.imgSize = 2160\n",
    "        \n",
    "        s.lpSmootherW = 0.1\n",
    "        s.normalSmootherW = 0.1\n",
    "\n",
    "        s.biLaplacian = False\n",
    "        s.jointRegularizerWeight = 0.000001\n",
    "        \n",
    "        s.kpFixingWeight = 1\n",
    "        \n",
    "class Renderer:\n",
    "    def __init__(s, cfg = RenderingCfg):\n",
    "        s.cfg = cfg\n",
    "        # blend_params = BlendParams(sigma=1e-4, gamma=1e-4)\n",
    "        s.blend_params = BlendParams(sigma=cfg.sigma, gamma=1e-4)\n",
    "\n",
    "        # Place a point light in front of the object. As mentioned above, the front of the cow is facing the \n",
    "        # -z direction. \n",
    "        s.lights = PointLights(device=device, location=[[0.0, 0.0, -3.0]])\n",
    "#         cameras = OpenGLPerspectiveCameras(device=device)\n",
    "        # Create a phong renderer by composing a rasterizer and a shader. The textured phong shader will \n",
    "        # interpolate the texture uv coordinates for each vertex, sample from a texture image and \n",
    "        # apply the Phong lighting model\n",
    "        \n",
    "        if cfg.blurRange!= 0:\n",
    "            s.raster_settings = RasterizationSettings(\n",
    "                image_size=cfg.imgSize, \n",
    "                blur_radius= np.log(1. / cfg.blurRange - 1.) * s.blend_params.sigma, \n",
    "                faces_per_pixel=cfg.faces_per_pixel, \n",
    "                bin_size=0\n",
    "            )\n",
    "        else:\n",
    "            s.raster_settings = RasterizationSettings(\n",
    "                image_size=cfg.imgSize, \n",
    "                blur_radius= 0, \n",
    "                faces_per_pixel=cfg.faces_per_pixel, \n",
    "                bin_size=0\n",
    "            )\n",
    "            \n",
    "        s.rasterizer=MeshRasterizer(\n",
    "                cameras=None, \n",
    "                raster_settings=s.raster_settings\n",
    "            )\n",
    "        if cfg.blurRange!= 0:\n",
    "            s.renderer = MeshRenderer(\n",
    "                rasterizer = s.rasterizer,\n",
    "            #     shader=SoftPhongShader(\n",
    "            #         device=device, \n",
    "            #         cameras=cameras,\n",
    "            #         lights=lights,\n",
    "            #         blend_params=blend_params\n",
    "            #     )\n",
    "                shader=SoftSilhouetteShader(\n",
    "                    blend_params=s.blend_params\n",
    "                    # device=device, \n",
    "                    # cameras=cameras,\n",
    "                    # lights=lights\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            s.renderer = MeshRenderer(\n",
    "                rasterizer = s.rasterizer,\n",
    "            #     shader=SoftPhongShader(\n",
    "            #         device=device, \n",
    "            #         cameras=cameras,\n",
    "            #         lights=lights,\n",
    "            #         blend_params=blend_params\n",
    "            #     )\n",
    "                shader=SoftSilhouetteShader(\n",
    "                    blend_params=s.blend_params\n",
    "                    # device=device, \n",
    "                    # cameras=cameras,\n",
    "                    # lights=lights\n",
    "                )\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = RenderingCfg()\n",
    "# cfg.sigma = 1e-3\n",
    "cfg.noiseLevel = 0.1\n",
    "\n",
    "# cfg.blurRange = 1e-1\n",
    "# cfg.sigma = 1e-4\n",
    "# cfg.sigma = 1e-5\n",
    "\n",
    "# cfg.blurRange = 1e-4\n",
    "\n",
    "# cfg.sigma = 1e-6\n",
    "# cfg.blurRange = 1e-5\n",
    "\n",
    "cfg.sigma = 1e-7\n",
    "cfg.blurRange = 1e-7\n",
    "\n",
    "# cfg.plotStep = 20\n",
    "cfg.plotStep = 20\n",
    "\n",
    "cfg.numCams = 16\n",
    "# cfg.learningRate = 1\n",
    "# cfg.learningRate = 0.1\n",
    "# cfg.learningRate = 1\n",
    "# cfg.learningRate = 0.002\n",
    "cfg.learningRate = 0.002\n",
    "\n",
    "# cfg.learningRate = 0.01\n",
    "# cfg.learningRate = 0.2\n",
    "\n",
    "# cfg.normalShiftLevel = 10\n",
    "cfg.normalShiftLevel = -3\n",
    "\n",
    "# cfg.faces_per_pixel = 14\n",
    "# cfg.faces_per_pixel = 30\n",
    "# cfg.faces_per_pixel = 15\n",
    "cfg.faces_per_pixel = 5\n",
    "\n",
    "# cfg.imgSize = 2160   \n",
    "cfg.imgSize = 1080\n",
    "device = torch.device(\"cuda:0\")\n",
    "cfg.terminateLoss = 0.1\n",
    "\n",
    "# cfg.lpSmootherW = 0.001\n",
    "\n",
    "cfg.lpSmootherW = 0.0001\n",
    "cfg.kpFixingWeight = 0.0001\n",
    "\n",
    "# cfg.normalSmootherW = 0.1\n",
    "cfg.normalSmootherW = 0.1\n",
    "\n",
    "renderSynth = Renderer(cfg)\n",
    "\n",
    "cfgRef = RenderingCfg()\n",
    "cfgRef.faces_per_pixel = 1\n",
    "cfgRef.blurRange = 0\n",
    "cfgRef.sigma = 0\n",
    "cfgRef.imgSize = 1080\n",
    "renderRef = Renderer(cfgRef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expName = 'PoseFitting_HeadKP__Sig' + str(cfg.sigma) + '_BR' + str(cfg.blurRange) + '_Fpp' + str(cfg.faces_per_pixel) \\\n",
    "+ '_NCams' + str(cfg.numCams)+ '_ImS' + str(cfg.imgSize) + '_LR' + str(cfg.learningRate) +'_JR' + str(cfg.jointRegularizerWeight) + '_KPW' + str(cfg.kpFixingWeight)\n",
    "\n",
    "outFolderForExperiment = join(outFolder, expName)\n",
    "os.makedirs(outFolderForExperiment, exist_ok=True)\n",
    "print(outFolderForExperiment)\n",
    "\n",
    "json.dump({\"CfgSynth\":cfg.__dict__, \"CfgRef\":cfgRef.__dict__,}, open(join(outFolderForExperiment, 'cfg.json'), 'w'), indent=2)\n",
    "\n",
    "outFolderMesh = join(outFolderForExperiment, 'Mesh')\n",
    "os.makedirs(outFolderMesh, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camRTs = []\n",
    "for iCam in range(cfg.numCams):\n",
    "    R, T = look_at_view_transform(2.7, 0, 360 * iCam / cfg.numCams, device=device) \n",
    "    camRTs.append({'R':R, 'T':T})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "images = []\n",
    "with torch.no_grad():\n",
    "    for iCam in range(len(cams)):\n",
    "        image_cur = renderSynth.renderer(smplshMesh,  cameras=cams[iCam])\n",
    "        images.append(image_cur.cpu().detach().numpy())\n",
    "visualize2DSilhouetteResults(images, backGroundImages = crops_out, outImgFile=join(outFolderForExperiment, 'Initial.png'))\n",
    "showCudaMemUsage(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_cur.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     loss = torch.sum((imageRef[..., 3] - image[..., 3]) ** 2)\n",
    "# print('Inital loss:', loss)\n",
    "poses = []\n",
    "losses = []\n",
    "\n",
    "optimizer = torch.optim.Adam([trans, pose, betas], lr=cfg.learningRate)\n",
    "\n",
    "logFile = join(outFolderForExperiment, 'Logs.txt')\n",
    "logger = Logger.configLogger(logFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smplshRegressorMatHead = torch.tensor(smplshRegressorMatHead,  dtype=torch.float32, device=device, requires_grad=False)\n",
    "headKps = torch.tensor( Keypoints.points[OPHeadKeypoints, :],  dtype=torch.float32, device=device, requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cfg.plotStep = 5\n",
    "cfg.numIterations = 300\n",
    "loop = tqdm_notebook(range(cfg.numIterations))\n",
    "\n",
    "for i in loop:\n",
    "    optimizer.zero_grad()\n",
    "#     torch.cuda.empty_cache()\n",
    "\n",
    "    lossVal = 0\n",
    "    for iCam in range(cfg.numCams):\n",
    "        refImg = torch.tensor(crops_out[iCam][..., 0], dtype=torch.float32, device=device, requires_grad=False)\n",
    "        verts = smplsh(betas, pose, trans).type(torch.float32) * 1000\n",
    "        smplshMesh = Meshes([verts], [smplsh.faces.to(device)])\n",
    "        \n",
    "        images = renderSynth.renderer(smplshMesh, cameras=cams[iCam])\n",
    "#         print(images.requires_grad)\n",
    "#         print(modifiedVerts.requires_grad)\n",
    "        loss = 1 - torch.norm(refImg * images[..., 3], p=1) / torch.norm(refImg + images[..., 3] - refImg * images[..., 3], p=1)\n",
    "        \n",
    "        loss.backward()\n",
    "        lossVal += loss.item()\n",
    "        #showCudaMemUsage(device)\n",
    "    \n",
    "#     mesh = Meshes(\n",
    "#                 verts=[modifiedVerts],   \n",
    "#                 faces=[faces_idx], \n",
    "# #                 textures=textures.to(device)\n",
    "#             )\n",
    "#     loss = cfg.normalSmootherW * mesh_normal_consistency(mesh)\n",
    "    \n",
    "# #     print(\"Laplacian on normal shift\",  cfg.lpSmootherW * normalShift.transpose(0,1) @ LNP @ normalShift)\n",
    "    \n",
    "#     # Laplacian on shift in normal direction\n",
    "#     loss = loss + cfg.lpSmootherW * normalShift.transpose(0,1) @ LNP @ normalShift\n",
    "    \n",
    "#     loss.backward()\n",
    "#     lossVal += loss.item()\n",
    "    # targetImg = images[0, ..., :3]\n",
    "    # loss, _ = model()\n",
    "    \n",
    "    # joint regularizer\n",
    "    loss = cfg.jointRegularizerWeight * torch.sum((pose**2))\n",
    "    loss.backward()\n",
    "    \n",
    "    # recordData\n",
    "    \n",
    "    verts = smplsh(betas, pose, trans).type(torch.float32) * 1000\n",
    "    headJoints = smplshRegressorMatHead @ verts\n",
    "    loss = cfg.kpFixingWeight * torch.sum((headJoints - headKps)**2)\n",
    "    loss.backward()\n",
    "    headKpFixingLoss = loss.item()\n",
    "                     \n",
    "    losses.append(lossVal)\n",
    "    \n",
    "    if i:\n",
    "        optimizer.step()\n",
    "        \n",
    "    memStats = torch.cuda.memory_stats(device=device)\n",
    "    memAllocated =  memStats['active_bytes.all.current'] / 1000000\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    infoStr = 'loss %.6f, headKpFixingLoss %.4f, MemUsed:%.2f' \\\n",
    "        % (lossVal, headKpFixingLoss, memAllocated)\n",
    "    \n",
    "    loop.set_description(infoStr)\n",
    "    logger.info(infoStr)\n",
    "    \n",
    "    #if lossVal < cfg.terminateLoss:\n",
    "    #    break\n",
    "    \n",
    "    # Save outputs to create a GIF. \n",
    "    if i % cfg.plotStep == 0:\n",
    "        showCudaMemUsage(device)\n",
    "        verts = smplsh(betas, pose, trans).type(torch.float32) * 1000\n",
    "        smplshMesh = Meshes([verts], [smplsh.faces.to(device)])\n",
    "\n",
    "        plt.close('all')\n",
    "        \n",
    "        outImgFile = join(outFolderForExperiment, 'Fig_' + str(i).zfill(5) + '.png')\n",
    "        renderedImages = []\n",
    "        with torch.no_grad():\n",
    "            for iCam in range(len(cams)):\n",
    "                image_cur = renderSynth.renderer(smplshMesh,  cameras=cams[iCam])\n",
    "#                 images.append(image_cur.cpu().detach().numpy())\n",
    "#                 imgDiff = np.abs(image_cur.cpu().detach().numpy() - crops_out[iCam][..., 0])\n",
    "                renderedImages.append(image_cur.cpu().detach().numpy())\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "#             showCudaMemUsage(device)\n",
    "#         visualize2DResults(diffImages, outImgFile=outImgFile, sizeInInches=5)\n",
    "        visualize2DSilhouetteResults(renderedImages, backGroundImages = crops_out, outImgFile=outImgFile, sizeInInches=5)\n",
    "        \n",
    "        saveVTK(join(outFolderMesh, 'Fit' + str(i).zfill(5) + '.ply'), verts.cpu().detach().numpy(), smplshExampleMesh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transVal = trans.cpu().detach().numpy()\n",
    "poseVal = pose.cpu().detach().numpy()\n",
    "betaVal = betas.cpu().detach().numpy()\n",
    "\n",
    "outParamFile = join(outFolderForExperiment,  'FittingParam.npz')\n",
    "np.savez(outParamFile, trans = transVal, pose=poseVal, beta=betaVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('pytorch3d': conda)",
   "language": "python",
   "name": "python361064bitpytorch3dcondaa21a2fd120614ce1afb4945f7b14ac5a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
